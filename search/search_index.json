{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PyChebyshev","text":"<p>Fast multi-dimensional Chebyshev tensor interpolation with analytical derivatives.</p> <p>PyChebyshev builds a Chebyshev interpolant of any smooth function in up to N dimensions, then evaluates it and its derivatives in microseconds using vectorized NumPy operations. Three classes cover different use cases:</p> <ul> <li><code>ChebyshevApproximation</code> \u2014 full tensor interpolation with analytical derivatives (up to ~5 dimensions)</li> <li><code>ChebyshevTT</code> \u2014 Tensor Train format via TT-Cross for 5+ dimensions</li> <li><code>ChebyshevSlider</code> \u2014 additive decomposition for separable high-dimensional functions</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Spectral accuracy \u2014 exponential error decay as node count increases</li> <li>Analytical derivatives \u2014 via spectral differentiation matrices (no finite differences)</li> <li>Tensor Train \u2014 TT-Cross builds from O(d\u00b7n\u00b7r\u00b2) evaluations instead of O(n^d)</li> <li>Fast evaluation \u2014 ~0.065 ms per query (price), ~0.29 ms for price + 5 Greeks</li> <li>Save &amp; load \u2014 persist built interpolants to disk; rebuild-free deployment</li> <li>Pure Python \u2014 NumPy + SciPy only, no compiled extensions needed</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import math\nfrom pychebyshev import ChebyshevApproximation\n\n# Define any smooth function\ndef my_func(x, _):\n    return math.sin(x[0]) * math.exp(-x[1])\n\n# Build interpolant\ncheb = ChebyshevApproximation(\n    my_func,\n    num_dimensions=2,\n    domain=[[-1, 1], [0, 2]],\n    n_nodes=[15, 15],\n)\ncheb.build()\n\n# Evaluate\nvalue = cheb.vectorized_eval([0.5, 1.0], [0, 0])\n\n# First derivative with respect to x[0]\ndfdx = cheb.vectorized_eval([0.5, 1.0], [1, 0])\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pychebyshev\n</code></pre>"},{"location":"#performance","title":"Performance","text":"Method Price Error Greek Error Build Time Query Time Chebyshev Barycentric 0.000% 1.980% ~0.35s ~0.065ms Chebyshev TT 0.014% 0.029% ~0.35s ~0.004ms MoCaX Standard (C++) 0.000% 1.980% ~1.04s ~0.47ms FDM 0.803% 2.234% N/A ~500ms <p>Based on 5D Black-Scholes tests with 11 nodes per dimension. TT uses ~7,400 function evaluations (vs 161,051 for full tensor methods). See Benchmarks for detailed comparisons including MoCaX TT.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"benchmarks/#5d-black-scholes-performance","title":"5D Black-Scholes Performance","text":"<p>All benchmarks use 5D Black-Scholes: \\(V(S, K, T, \\sigma, r)\\) with 11 Chebyshev nodes per dimension (\\(11^5 = 161{,}051\\) grid points).</p>"},{"location":"benchmarks/#accuracy","title":"Accuracy","text":"Method Price Error Greek Error (max) Chebyshev Barycentric 0.000% 1.980% MoCaX Standard (C++) 0.000% 1.980% FDM (Crank-Nicolson) 0.803% 2.234% <p>Both Chebyshev methods achieve machine-precision price accuracy and identical Greek errors, as they compute the same unique interpolating polynomial.</p>"},{"location":"benchmarks/#timing","title":"Timing","text":"Method Build Time Price Query Price + 5 Greeks Chebyshev Barycentric 0.35s 0.065 ms 0.29 ms MoCaX Standard (C++) 1.04s 0.47 ms 2.85 ms Analytical (direct) N/A 0.01 ms 0.06 ms"},{"location":"benchmarks/#evaluation-method-comparison","title":"Evaluation Method Comparison","text":"<p>Within PyChebyshev, multiple evaluation paths exist:</p> Method Price Only Price + 5 Greeks Notes <code>eval()</code> ~45 ms ~270 ms Python loops, full validation <code>fast_eval()</code> ~10 ms ~95 ms Deprecated \u2014 JIT scalar loops <code>vectorized_eval()</code> 0.065 ms 0.39 ms Recommended \u2014 BLAS GEMV <code>vectorized_eval_multi()</code> \u2014 0.29 ms Shared weights across derivatives <p><code>vectorized_eval()</code> is the recommended default. Use <code>vectorized_eval_multi()</code> when computing multiple derivatives at the same point.</p> <p>Why BLAS beats JIT</p> <p><code>fast_eval()</code> uses Numba JIT to compile scalar barycentric interpolation loops. But <code>vectorized_eval()</code> restructures the algorithm into matrix-vector products (BLAS GEMV), replacing 16,105 Python loop iterations with 5 BLAS calls for a 5D problem. Optimized BLAS (OpenBLAS/MKL) running a single GEMV is fundamentally faster than JIT-compiled scalar loops \u2014 the data access pattern is more cache-friendly and leverages SIMD vectorization at the hardware level. <code>fast_eval()</code> is deprecated and will be removed in a future version.</p>"},{"location":"benchmarks/#tensor-train-tt-vs-mocax-extend","title":"Tensor Train (TT) vs MoCaX Extend","text":"<p><code>ChebyshevTT</code> and MoCaX <code>MocaxExtend</code> both build Chebyshev interpolants in TT format for the same 5D Black-Scholes problem. PyChebyshev uses TT-Cross (maxvol pivoting); MoCaX uses rank-adaptive ALS on a random subgrid. Both use ~7,400--8,000 function evaluations.</p>"},{"location":"benchmarks/#build","title":"Build","text":"Metric PyChebyshev TT MoCaX TT Build time 0.35s 5.73s Function evaluations 7,419 8,000 TT ranks [1, 11, 11, 11, 7, 1] (not exposed) Compression ratio 43.4x N/A"},{"location":"benchmarks/#price-accuracy-50-random-test-points","title":"Price Accuracy (50 random test points)","text":"Metric PyChebyshev TT MoCaX TT Mean error 0.002% 0.093% Max error 0.014% 0.712% Median error 0.001% 0.045%"},{"location":"benchmarks/#evaluation-speed-1000-random-points","title":"Evaluation Speed (1000 random points)","text":"Method PyChebyshev TT MoCaX TT Single eval 0.065 ms -- Batch eval 0.004 ms 0.246 ms"},{"location":"benchmarks/#greeks-accuracy-10-scenarios-fd-vs-analytical","title":"Greeks Accuracy (10 scenarios, FD vs analytical)","text":"Greek PyChebyshev avg error MoCaX avg error Delta 0.029% 0.379% Gamma 0.019% 1.604% <p>To reproduce: <code>uv run --with tqdm --with blackscholes python compare_tensor_train.py</code> (requires MoCaX C++ library; PyChebyshev results are shown regardless).</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#from-pypi","title":"From PyPI","text":"<pre><code>pip install pychebyshev\n</code></pre>"},{"location":"getting-started/#from-source-development","title":"From source (development)","text":"<pre><code>git clone https://github.com/0xC000005/PyChebyshev.git\ncd PyChebyshev\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#1-define-your-function","title":"1. Define your function","text":"<p>PyChebyshev can approximate any smooth function. The function signature is <code>f(point, data) -&gt; float</code>, where <code>point</code> is a list of coordinates and <code>data</code> is optional additional data (pass <code>None</code> if unused).</p> <pre><code>import math\n\ndef my_func(x, _):\n    return math.sin(x[0]) + math.cos(x[1])\n</code></pre>"},{"location":"getting-started/#2-build-the-interpolant","title":"2. Build the interpolant","text":"<pre><code>from pychebyshev import ChebyshevApproximation\n\ncheb = ChebyshevApproximation(\n    function=my_func,\n    num_dimensions=2,\n    domain=[[-3, 3], [-3, 3]],  # bounds per dimension\n    n_nodes=[15, 15],            # Chebyshev nodes per dimension\n)\ncheb.build()\n</code></pre>"},{"location":"getting-started/#3-evaluate","title":"3. Evaluate","text":"<pre><code># Function value\nvalue = cheb.vectorized_eval([1.0, 2.0], [0, 0])\n\n# First derivative w.r.t. x[0]\ndfdx0 = cheb.vectorized_eval([1.0, 2.0], [1, 0])\n\n# Second derivative w.r.t. x[1]\nd2fdx1 = cheb.vectorized_eval([1.0, 2.0], [0, 2])\n</code></pre>"},{"location":"getting-started/#4-evaluate-price-all-greeks-at-once","title":"4. Evaluate price + all Greeks at once","text":"<p>For maximum efficiency when computing multiple derivatives at the same point:</p> <pre><code>results = cheb.vectorized_eval_multi(\n    [1.0, 2.0],\n    [\n        [0, 0],  # function value\n        [1, 0],  # df/dx0\n        [0, 1],  # df/dx1\n        [2, 0],  # d2f/dx0^2\n    ],\n)\n# results = [value, dfdx0, dfdx1, d2fdx0]\n</code></pre> <p>This shares barycentric weights across all derivative orders, saving ~25% compared to separate calls.</p>"},{"location":"getting-started/#5-save-for-later","title":"5. Save for later","text":"<p>Save the built interpolant to skip rebuilding next time:</p> <pre><code>cheb.save(\"my_interpolant.pkl\")\n</code></pre> <p>Load it back \u2014 no rebuild needed:</p> <pre><code>from pychebyshev import ChebyshevApproximation\n\ncheb = ChebyshevApproximation.load(\"my_interpolant.pkl\")\nvalue = cheb.vectorized_eval([1.0, 2.0], [0, 0])\n</code></pre> <p>See Saving &amp; Loading for details.</p>"},{"location":"getting-started/#choosing-node-counts","title":"Choosing Node Counts","text":"<ul> <li>10-15 nodes per dimension is typical for smooth analytic functions</li> <li>More nodes = higher accuracy but more build-time evaluations (\\(n_1 \\times n_2 \\times \\cdots\\))</li> <li>For 5D with 11 nodes: \\(11^5 = 161{,}051\\) function evaluations at build time</li> <li>Convergence is exponential for analytic functions \u2014 a few extra nodes can eliminate errors entirely</li> </ul>"},{"location":"getting-started/#choosing-the-right-class","title":"Choosing the Right Class","text":"Class Dimensions Build Cost Derivatives Best For <code>ChebyshevApproximation</code> 1\u20135 \\(n^d\\) evals Analytical Full accuracy with spectral derivatives <code>ChebyshevTT</code> 5+ \\(O(d \\cdot n \\cdot r^2)\\) evals Finite differences High-dimensional problems where full grids are infeasible <code>ChebyshevSlider</code> 5+ Sum of slide grids Analytical (per slide) Functions with additive/separable structure"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Computing Greeks -- analytical derivatives for pricing</li> <li>Error Estimation -- validate accuracy without test points</li> <li>Saving &amp; Loading -- persist built interpolants</li> <li>Benchmarks -- performance comparison with MoCaX C++</li> </ul>"},{"location":"api/reference/","title":"API Reference","text":""},{"location":"api/reference/#chebyshevapproximation","title":"ChebyshevApproximation","text":"<p>Multi-dimensional Chebyshev approximation using barycentric interpolation.</p> <p>Pre-computes barycentric weights for all dimensions at build time, enabling uniform O(N) evaluation complexity for every dimension. Supports analytical derivatives via spectral differentiation matrices.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>callable</code> <p>Function to approximate. Signature: <code>f(point, data) -&gt; float</code> where <code>point</code> is a list of floats and <code>data</code> is arbitrary additional data (can be None).</p> required <code>num_dimensions</code> <code>int</code> <p>Number of input dimensions.</p> required <code>domain</code> <code>list of (float, float)</code> <p>Bounds [(lo, hi), ...] for each dimension.</p> required <code>n_nodes</code> <code>list of int</code> <p>Number of Chebyshev nodes per dimension.</p> required <code>max_derivative_order</code> <code>int</code> <p>Maximum derivative order to support. Default is 2.</p> <code>2</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; def f(x, _):\n...     return math.sin(x[0]) + math.sin(x[1])\n&gt;&gt;&gt; cheb = ChebyshevApproximation(f, 2, [[-1, 1], [-1, 1]], [11, 11])\n&gt;&gt;&gt; cheb.build()\n&gt;&gt;&gt; cheb.vectorized_eval([0.5, 0.3], [0, 0])\n0.7764...\n</code></pre>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.build","title":"<code>build(verbose=True)</code>","text":"<p>Evaluate the function at all node combinations and pre-compute weights.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print build progress. Default is True.</p> <code>True</code>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.eval","title":"<code>eval(point, derivative_order)</code>","text":"<p>Evaluate using dimensional decomposition with barycentric interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Query point, one coordinate per dimension.</p> required <code>derivative_order</code> <code>list of int</code> <p>Derivative order per dimension (0 = function value, 1 = first derivative, 2 = second derivative).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Interpolated value or derivative at the query point.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.fast_eval","title":"<code>fast_eval(point, derivative_order)</code>","text":"<p>Fast evaluation using pre-allocated cache (skips validation).</p> <p>.. deprecated:: 0.3.0     Use :meth:<code>vectorized_eval</code> instead, which is ~150x faster via     BLAS GEMV and requires no optional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Query point.</p> required <code>derivative_order</code> <code>list of int</code> <p>Derivative order per dimension.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Interpolated value or derivative.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.vectorized_eval","title":"<code>vectorized_eval(point, derivative_order)</code>","text":"<p>Fully vectorized evaluation using NumPy matrix operations.</p> <p>Replaces the Python loop with BLAS matrix-vector products. For 5-D with 11 nodes: 5 BLAS calls instead of 16,105 Python iterations.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Query point, one coordinate per dimension.</p> required <code>derivative_order</code> <code>list of int</code> <p>Derivative order per dimension.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Interpolated value or derivative.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.vectorized_eval_batch","title":"<code>vectorized_eval_batch(points, derivative_order)</code>","text":"<p>Evaluate at multiple points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>Points of shape (N, num_dimensions).</p> required <code>derivative_order</code> <code>list of int</code> <p>Derivative order per dimension.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Results of shape (N,).</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.vectorized_eval_multi","title":"<code>vectorized_eval_multi(point, derivative_orders)</code>","text":"<p>Evaluate multiple derivative orders at the same point, sharing weights.</p> <p>Pre-computes normalized barycentric weights once per dimension and reuses them across all derivative orders. Computing price + 5 Greeks costs ~0.29 ms instead of 6 x 0.065 ms = 0.39 ms.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Query point.</p> required <code>derivative_orders</code> <code>list of list of int</code> <p>Each inner list specifies derivative order per dimension.</p> required <p>Returns:</p> Type Description <code>list of float</code> <p>One result per derivative order.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.get_derivative_id","title":"<code>get_derivative_id(derivative_order)</code>","text":"<p>Return derivative order as-is (for API compatibility).</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.error_estimate","title":"<code>error_estimate()</code>","text":"<p>Estimate the supremum-norm interpolation error.</p> <p>Computes Chebyshev expansion coefficients via DCT-II for each 1-D slice of the tensor, and returns the sum of per-dimension maximum last-coefficient magnitudes:</p> <p>.. math::</p> <pre><code>\\hat{E} = \\sum_{d=1}^{D}\n    \\max_{\\text{slices along } d} |c_{n_d - 1}|\n</code></pre> <p>This follows the ex ante error estimation from Ruiz &amp; Zeron (2021), Section 3.4, adapted for Type I Chebyshev nodes.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated maximum interpolation error (sup-norm).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Return picklable state, excluding the original function and eval cache.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore state and reconstruct the eval cache.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.save","title":"<code>save(path)</code>","text":"<p>Save the built interpolant to a file.</p> <p>The original function is not saved \u2014 only the numerical data needed for evaluation. The saved file can be loaded with :meth:<code>load</code> without access to the original function.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Destination file path.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the interpolant has not been built yet.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevApproximation.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load a previously saved interpolant from a file.</p> <p>The loaded object can evaluate immediately; no rebuild is needed. The <code>function</code> attribute will be <code>None</code>. Assign a new function before calling <code>build()</code> again if a rebuild is desired.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Path to the saved file.</p> required <p>Returns:</p> Type Description <code>ChebyshevApproximation</code> <p>The restored interpolant.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the file was saved with a different PyChebyshev version.</p> <code>.. warning::</code> <p>This method uses :mod:<code>pickle</code> internally. Pickle can execute arbitrary code during deserialization. Only load files you trust.</p>"},{"location":"api/reference/#chebyshevslider","title":"ChebyshevSlider","text":"<p>Chebyshev Sliding approximation for high-dimensional functions.</p> <p>Decomposes f(x_1, ..., x_n) into a sum of low-dimensional Chebyshev interpolants (slides) around a pivot point z:</p> <pre><code>f(x) \u2248 f(z) + \u03a3_i [s_i(x_group_i) - f(z)]\n</code></pre> <p>where each slide s_i is a ChebyshevApproximation built on a subset of dimensions with the remaining dimensions fixed at z.</p> <p>This trades accuracy for dramatically reduced build cost: instead of evaluating f at n_1 \u00d7 n_2 \u00d7 ... \u00d7 n_d grid points (exponential), the slider evaluates at n_1 \u00d7 n_2 + n_3 \u00d7 n_4 + ... (sum of products within each group).</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>callable</code> <p>Function to approximate. Signature: <code>f(point, data) -&gt; float</code> where <code>point</code> is a list of floats and <code>data</code> is arbitrary additional data (can be None).</p> required <code>num_dimensions</code> <code>int</code> <p>Total number of input dimensions.</p> required <code>domain</code> <code>list of (float, float)</code> <p>Bounds [lo, hi] for each dimension.</p> required <code>n_nodes</code> <code>list of int</code> <p>Number of Chebyshev nodes per dimension.</p> required <code>partition</code> <code>list of list of int</code> <p>Grouping of dimension indices into slides. Each dimension must appear in exactly one group. E.g. <code>[[0,1,2], [3,4]]</code> creates a 3D slide for dims 0,1,2 and a 2D slide for dims 3,4.</p> required <code>pivot_point</code> <code>list of float</code> <p>Reference point z around which slides are built.</p> required <code>max_derivative_order</code> <code>int</code> <p>Maximum derivative order to pre-compute (default 2).</p> <code>2</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; def f(x, _):\n...     return math.sin(x[0]) + math.sin(x[1]) + math.sin(x[2])\n&gt;&gt;&gt; slider = ChebyshevSlider(\n...     f, 3, [[-1,1], [-1,1], [-1,1]], [11,11,11],\n...     partition=[[0], [1], [2]],\n...     pivot_point=[0.0, 0.0, 0.0],\n... )\n&gt;&gt;&gt; slider.build(verbose=False)\n&gt;&gt;&gt; round(slider.eval([0.5, 0.3, 0.1], [0,0,0]), 4)\n0.8764\n</code></pre>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.total_build_evals","title":"<code>total_build_evals</code>  <code>property</code>","text":"<p>Total number of function evaluations used during build.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.build","title":"<code>build(verbose=True)</code>","text":"<p>Build all slides by evaluating the function at slide-specific grids.</p> <p>For each slide, dimensions outside the slide group are fixed at their pivot values.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print build progress. Default is True.</p> <code>True</code>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.eval","title":"<code>eval(point, derivative_order)</code>","text":"<p>Evaluate the slider approximation at a point.</p> <p>Uses Equation 7.5 from Ruiz &amp; Zeron (2021):     f(x) \u2248 f(z) + \u03a3_i [s_i(x_i) - f(z)]</p> <p>For derivatives, only the slide containing that dimension contributes.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Evaluation point in the full n-dimensional space.</p> required <code>derivative_order</code> <code>list of int</code> <p>Derivative order for each dimension (0 = function value).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Approximated function value or derivative.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.eval_multi","title":"<code>eval_multi(point, derivative_orders)</code>","text":"<p>Evaluate slider at multiple derivative orders for the same point.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Evaluation point in the full n-dimensional space.</p> required <code>derivative_orders</code> <code>list of list of int</code> <p>Each inner list specifies derivative order per dimension.</p> required <p>Returns:</p> Type Description <code>list of float</code> <p>Results for each derivative order.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.error_estimate","title":"<code>error_estimate()</code>","text":"<p>Estimate the sliding approximation error.</p> <p>Returns the sum of per-slide Chebyshev error estimates. Each slide's error is estimated independently using the Chebyshev coefficient method from Ruiz &amp; Zeron (2021), Section 3.4.</p> <p>Note: This captures per-slide interpolation error only. Cross-group interaction error (inherent to the sliding decomposition) is not included.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated interpolation error (per-slide sum).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Return picklable state, excluding the original function.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore state from a pickled dict.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.save","title":"<code>save(path)</code>","text":"<p>Save the built slider to a file.</p> <p>The original function is not saved \u2014 only the numerical data needed for evaluation. The saved file can be loaded with :meth:<code>load</code> without access to the original function.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Destination file path.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the slider has not been built yet.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevSlider.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load a previously saved slider from a file.</p> <p>The loaded object can evaluate immediately; no rebuild is needed. The <code>function</code> attribute will be <code>None</code>. Assign a new function before calling <code>build()</code> again if a rebuild is desired.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Path to the saved file.</p> required <p>Returns:</p> Type Description <code>ChebyshevSlider</code> <p>The restored slider.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the file was saved with a different PyChebyshev version.</p> <code>.. warning::</code> <p>This method uses :mod:<code>pickle</code> internally. Pickle can execute arbitrary code during deserialization. Only load files you trust.</p>"},{"location":"api/reference/#chebyshevtt","title":"ChebyshevTT","text":"<p>Chebyshev interpolation in Tensor Train format.</p> <p>For functions of 5+ dimensions where full tensor interpolation is infeasible. Uses TT-Cross to build from O(d * n * r^2) function evaluations instead of O(n^d), then evaluates via TT inner product.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>callable</code> <p>Function to approximate. Signature: <code>f(point, data) -&gt; float</code> where <code>point</code> is a list of floats and <code>data</code> is arbitrary additional data (can be None).</p> required <code>num_dimensions</code> <code>int</code> <p>Number of input dimensions.</p> required <code>domain</code> <code>list of (float, float)</code> <p>Bounds [(lo, hi), ...] for each dimension.</p> required <code>n_nodes</code> <code>list of int</code> <p>Number of Chebyshev nodes per dimension.</p> required <code>max_rank</code> <code>int</code> <p>Maximum TT rank. Higher = more accurate, more expensive. Default is 10.</p> <code>10</code> <code>tolerance</code> <code>float</code> <p>Convergence tolerance for TT-Cross. Default is 1e-6.</p> <code>1e-06</code> <code>max_sweeps</code> <code>int</code> <p>Maximum number of TT-Cross sweeps. Default is 10.</p> <code>10</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; def f(x, _):\n...     return math.sin(x[0]) + math.sin(x[1]) + math.sin(x[2])\n&gt;&gt;&gt; tt = ChebyshevTT(f, 3, [[-1, 1], [-1, 1], [-1, 1]], [11, 11, 11])\n&gt;&gt;&gt; tt.build(verbose=False)\n&gt;&gt;&gt; tt.eval([0.5, 0.3, 0.1])\n0.8764...\n</code></pre>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.tt_ranks","title":"<code>tt_ranks</code>  <code>property</code>","text":"<p>TT ranks [1, r_1, r_2, ..., r_{d-1}, 1].</p> <p>Returns:</p> Type Description <code>list of int</code> <p>The TT rank vector. Only available after :meth:<code>build</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.compression_ratio","title":"<code>compression_ratio</code>  <code>property</code>","text":"<p>Ratio of full tensor elements to TT storage elements.</p> <p>Returns:</p> Type Description <code>float</code> <p>Compression ratio (&gt; 1 means TT is more compact).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.total_build_evals","title":"<code>total_build_evals</code>  <code>property</code>","text":"<p>Total number of function evaluations used during build.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of function evaluations. Only meaningful after :meth:<code>build</code>.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.build","title":"<code>build(verbose=True, seed=None, method='cross')</code>","text":"<p>Build TT approximation and convert to Chebyshev coefficient cores.</p> <p>The build process has three stages:</p> <ol> <li>Generate Chebyshev grids. Compute Type I Chebyshev nodes    in each dimension, scaled to the specified domain.</li> <li>Build value cores. Either TT-Cross (evaluating at    \\(O(d \\cdot n \\cdot r^2)\\) strategically selected points) or    TT-SVD (evaluating the full \\(O(n^d)\\) tensor, then decomposing    via sequential SVD).</li> <li>Convert to coefficient cores. Apply DCT-II along the node    axis of each core to convert from function values at Chebyshev    nodes to Chebyshev expansion coefficients. This enables    evaluation at arbitrary (non-grid) points via the Chebyshev    polynomial inner product.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print build progress. Default is True.</p> <code>True</code> <code>seed</code> <code>int or None</code> <p>Random seed for TT-Cross initialization. Default is None. Ignored when <code>method='svd'</code>.</p> <code>None</code> <code>method</code> <code>``'cross'`` or ``'svd'``</code> <p>Build algorithm. <code>'cross'</code> (default) uses TT-Cross to evaluate the function at \\(O(d \\cdot n \\cdot r^2)\\) strategically selected points. <code>'svd'</code> builds the full tensor and decomposes via truncated SVD -- only feasible for moderate dimensions (\\(d \\leq 6\\)) but useful for validation.</p> <code>'cross'</code>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.eval","title":"<code>eval(point)</code>","text":"<p>Evaluate at a single point via TT inner product.</p> <p>Computes the Chebyshev interpolant value at an arbitrary point by contracting the pre-computed coefficient cores with Chebyshev polynomial values. For each dimension \\(k\\):</p> <ol> <li>Scale the query coordinate to \\([-1, 1]\\).</li> <li>Evaluate all Chebyshev polynomials \\(T_0, \\ldots, T_{n_k-1}\\).</li> <li>Contract with the coefficient core:    \\(v = \\sum_j q_j \\cdot \\text{core}[:, j, :]\\)</li> </ol> <p>The chain of contractions reduces to a scalar. Cost: \\(O(d \\cdot n \\cdot r^2)\\) per point.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Query point, one coordinate per dimension.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Interpolated value at the query point.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.eval_batch","title":"<code>eval_batch(points)</code>","text":"<p>Evaluate at multiple points simultaneously.</p> <p>Vectorizes the TT inner product over all N points using <code>np.einsum</code> for batched matrix contractions. For each dimension, all N polynomial vectors are contracted with the coefficient core in a single einsum call, then all N chain multiplications proceed in parallel. Typical speedup is 15--20x over calling :meth:<code>eval</code> in a loop.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray of shape (N, num_dimensions)</code> <p>Query points.</p> required <p>Returns:</p> Type Description <code>ndarray of shape (N,)</code> <p>Interpolated values at each query point.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.eval_multi","title":"<code>eval_multi(point, derivative_orders)</code>","text":"<p>Evaluate with finite-difference derivatives at a single point.</p> <p>Uses central finite differences. The first entry in <code>derivative_orders</code> is typically <code>[0, 0, ..., 0]</code> for the function value; subsequent entries specify derivative orders per dimension.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Evaluation point in the full n-dimensional space.</p> required <code>derivative_orders</code> <code>list of list of int</code> <p>Each inner list specifies derivative order per dimension. Supports 0 (value), 1 (first derivative), and 2 (second derivative).</p> required <p>Returns:</p> Type Description <code>list of float</code> <p>One result per derivative order specification.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.error_estimate","title":"<code>error_estimate()</code>","text":"<p>Estimate interpolation error from Chebyshev coefficient cores.</p> <p>For each dimension d, takes the maximum magnitude of the last Chebyshev coefficient across all \"rows\" and \"columns\" of the core (i.e., max over left-rank and right-rank indices of <code>|core[:, -1, :]|</code>). Returns the sum across dimensions.</p> <p>This is an approximate analog of the ex ante error estimation from Ruiz &amp; Zeron (2021), Section 3.4, adapted for TT format.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated interpolation error.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Return picklable state, excluding the original function.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore state from a pickled dict.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.save","title":"<code>save(path)</code>","text":"<p>Save the built TT interpolant to a file.</p> <p>The original function is not saved -- only the numerical data needed for evaluation. The saved file can be loaded with :meth:<code>load</code> without access to the original function.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Destination file path.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"api/reference/#pychebyshev.ChebyshevTT.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load a previously saved TT interpolant from a file.</p> <p>The loaded object can evaluate immediately; no rebuild is needed. The <code>function</code> attribute will be <code>None</code>. Assign a new function before calling <code>build()</code> again if a rebuild is desired.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Path to the saved file.</p> required <p>Returns:</p> Type Description <code>ChebyshevTT</code> <p>The restored TT interpolant.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the file was saved with a different PyChebyshev version.</p> <code>.. warning::</code> <p>This method uses :mod:<code>pickle</code> internally. Pickle can execute arbitrary code during deserialization. Only load files you trust.</p>"},{"location":"api/reference/#module-functions","title":"Module Functions","text":"<p>Compute barycentric weights for given nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>ndarray</code> <p>Interpolation nodes of shape (n,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Barycentric weights w_i = 1 / prod_{j!=i} (x_i - x_j).</p> <p>Compute spectral differentiation matrix for barycentric interpolation.</p> <p>Based on Berrut &amp; Trefethen (2004), Section 9.3.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>ndarray</code> <p>Interpolation nodes of shape (n,).</p> required <code>weights</code> <code>ndarray</code> <p>Barycentric weights of shape (n,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Differentiation matrix D of shape (n, n) such that D @ f gives derivative values at nodes.</p> <p>Evaluate barycentric interpolation at a single point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Evaluation point.</p> required <code>nodes</code> <code>ndarray</code> <p>Interpolation nodes.</p> required <code>values</code> <code>ndarray</code> <p>Function values at nodes.</p> required <code>weights</code> <code>ndarray</code> <p>Barycentric weights.</p> required <code>skip_check</code> <code>bool</code> <p>If True, skip node coincidence check (faster but may divide by zero).</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Interpolated value p(x).</p>"},{"location":"user-guide/concepts/","title":"Mathematical Concepts","text":""},{"location":"user-guide/concepts/#why-chebyshev-interpolation","title":"Why Chebyshev Interpolation?","text":"<p>Polynomial interpolation with equally-spaced points suffers from Runge's phenomenon \u2014 wild oscillations near interval endpoints that worsen as polynomial degree increases. Chebyshev nodes solve this by clustering near boundaries:</p> \\[x_i = \\cos\\left(\\frac{(2i-1)\\pi}{2n}\\right), \\quad i = 1, \\ldots, n\\] <p>The Lebesgue constant for Chebyshev nodes grows only logarithmically: \\(\\Lambda_n \\leq \\frac{2}{\\pi}\\log(n+1) + 1\\), versus exponential growth for equidistant points.</p>"},{"location":"user-guide/concepts/#spectral-convergence","title":"Spectral Convergence","text":"<p>For functions analytic in a Bernstein ellipse with parameter \\(\\rho &gt; 1\\), the interpolation error decays exponentially:</p> \\[|f(x) - p_N(x)| = O(\\rho^{-N})\\] <p>Each additional node multiplies accuracy by a constant factor \\(\\rho\\).</p>"},{"location":"user-guide/concepts/#bernstein-ellipse","title":"Bernstein ellipse","text":"<p>A Bernstein ellipse is an ellipse in the complex plane with foci at \\(x = -1\\) and \\(x = +1\\). The parameter \\(\\rho\\) equals the sum of the semi-major and semi-minor axis lengths. Functions analytic inside a larger ellipse (larger \\(\\rho\\)) converge faster.</p> <p>Practical implication: The convergence rate depends on how far the function's nearest singularity (pole, branch cut, discontinuity) is from the real interval \\([-1, 1]\\) in the complex plane. For example:</p> <ul> <li>\\(f(x) = e^x\\) is entire (no singularities) -- \\(\\rho = \\infty\\), superexponential convergence.</li> <li>\\(f(x) = 1/(1 + 25x^2)\\) has poles at \\(x = \\pm i/5\\) -- the Bernstein ellipse must   avoid these poles, limiting \\(\\rho\\) and slowing convergence.</li> <li>Black-Scholes option prices are analytic in all parameters over typical domains,   giving large \\(\\rho\\) and rapid convergence with 10--15 nodes per dimension.</li> </ul> <p>For the full theory, see Trefethen, Approximation Theory and Approximation Practice, SIAM 2019, Chapter 8.</p>"},{"location":"user-guide/concepts/#barycentric-interpolation-formula","title":"Barycentric Interpolation Formula","text":"<p>The interpolating polynomial is expressed as:</p> \\[p(x) = \\frac{\\sum_{i=0}^{n} \\frac{w_i f_i}{x - x_i}}{\\sum_{i=0}^{n} \\frac{w_i}{x - x_i}}\\] <p>where the barycentric weights \\(w_i = 1 / \\prod_{j \\neq i}(x_i - x_j)\\) depend only on node positions, not on function values. This enables full pre-computation.</p>"},{"location":"user-guide/concepts/#multi-dimensional-extension","title":"Multi-Dimensional Extension","text":"<p>For a \\(d\\)-dimensional function, PyChebyshev uses dimensional decomposition:</p> <ol> <li>Start with the full tensor of function values at all node combinations</li> <li>Contract one dimension at a time using barycentric interpolation</li> <li>Each contraction reduces dimensionality by 1 (5D \u2192 4D \u2192 ... \u2192 scalar)</li> </ol> <p>This avoids the curse of dimensionality in the evaluation step \u2014 query cost scales linearly with the number of dimensions.</p>"},{"location":"user-guide/concepts/#analytical-derivatives","title":"Analytical Derivatives","text":"<p>Derivatives are computed using spectral differentiation matrices \\(D^{(k)}\\):</p> \\[D^{(1)}_{ij} = \\frac{w_j / w_i}{x_i - x_j} \\quad (i \\neq j), \\qquad D^{(1)}_{ii} = -\\sum_{k \\neq i} D^{(1)}_{ik}\\] <p>Given function values \\(\\mathbf{f}\\) at nodes, \\(D^{(1)} \\mathbf{f}\\) gives exact derivative values at those same nodes. These derivative values are then interpolated to the query point using the barycentric formula.</p>"},{"location":"user-guide/error-estimation/","title":"Error Estimation","text":""},{"location":"user-guide/error-estimation/#introduction","title":"Introduction","text":"<p>After building a Chebyshev interpolant, you often want to know how accurate it is without comparing against the true function at thousands of test points. <code>error_estimate()</code> provides an ex ante estimate of the interpolation error using only the Chebyshev coefficients already computed during the build step.</p> <p>This is useful for:</p> <ul> <li>Validating node counts -- check whether your grid is fine enough before deploying.</li> <li>Adaptive refinement -- increase nodes in dimensions where the error is large.</li> <li>Confidence reporting -- attach an approximate error magnitude to interpolated values.</li> </ul>"},{"location":"user-guide/error-estimation/#quick-start","title":"Quick Start","text":"<pre><code>from pychebyshev import ChebyshevApproximation\nimport math\n\ndef f(x, _):\n    return math.sin(x[0]) * math.cos(x[1])\n\ncheb = ChebyshevApproximation(f, 2, [[-1, 1], [-1, 1]], [15, 15])\ncheb.build(verbose=False)\nprint(f\"Error estimate: {cheb.error_estimate():.2e}\")\n</code></pre> <p>No extra function evaluations are needed -- the estimate is computed from the tensor of function values that <code>build()</code> already stored.</p>"},{"location":"user-guide/error-estimation/#mathematical-background","title":"Mathematical Background","text":""},{"location":"user-guide/error-estimation/#chebyshev-series-expansion","title":"Chebyshev series expansion","text":"<p>Any sufficiently smooth function on \\([-1, 1]\\) can be expanded in Chebyshev polynomials:</p> \\[f(x) = \\sum_{k=0}^{\\infty} c_k\\, T_k(x)\\] <p>where \\(T_k\\) is the Chebyshev polynomial of the first kind of degree \\(k\\), and \\(c_k\\) are the expansion coefficients. When we interpolate with \\(n\\) nodes, we compute a degree-\\((n-1)\\) polynomial that implicitly truncates this series:</p> \\[p_n(x) = \\sum_{k=0}^{n-1} \\hat{c}_k\\, T_k(x)\\] <p>The interpolation error \\(f(x) - p_n(x)\\) comes from two sources: (1) the omitted high-degree terms \\(c_n, c_{n+1}, \\ldots\\) and (2) aliasing, where these omitted terms fold back onto the computed coefficients. For well-resolved functions, both sources are small when the trailing coefficients are small.</p>"},{"location":"user-guide/error-estimation/#why-the-last-coefficient-estimates-the-error","title":"Why the last coefficient estimates the error","text":"<p>For a function analytic in a Bernstein ellipse with parameter \\(\\rho &gt; 1\\) (see Mathematical Concepts), the Chebyshev coefficients satisfy \\(|c_k| = O(\\rho^{-k})\\). This means each successive coefficient is roughly \\(\\rho\\) times smaller than the previous one. The last included coefficient \\(|c_{n-1}|\\) is therefore:</p> <ol> <li> <p>An upper bound on the omitted tail. Since \\(|c_k| \\leq M \\rho^{-k}\\) and the    tail sum \\(\\sum_{k=n}^{\\infty} |c_k|\\) is a geometric series with ratio    \\(\\rho^{-1} &lt; 1\\), we have    \\(\\sum_{k=n}^{\\infty} |c_k| \\lesssim |c_{n-1}| / (\\rho - 1)\\). When    \\(\\rho\\) is even moderately large (say, \\(\\rho &gt; 2\\)), the omitted tail is    comparable in magnitude to \\(|c_{n-1}|\\) itself.</p> </li> <li> <p>A proxy for aliasing error. The aliased contributions (omitted terms folding    onto lower coefficients) are bounded by the same geometric decay, so they are also    \\(O(|c_{n-1}|)\\) for well-resolved functions.</p> </li> </ol> <p>The practical rule: if \\(|c_{n-1}|\\) is small, both the truncation and aliasing errors are small, and the interpolant is well-converged.</p> <p>Heuristic, not a formal bound</p> <p>This estimate is an empirically reliable proxy, not a rigorous upper bound. Ruiz &amp; Zeron (2021, Section 3.4) report that they have never encountered a real-world case where small trailing coefficients failed to indicate convergence. However, pathological functions (e.g., those with singularities just outside the Bernstein ellipse) could have slowly decaying coefficients that make the estimate optimistic. Always validate against known solutions when possible.</p>"},{"location":"user-guide/error-estimation/#computing-coefficients-via-dct-ii","title":"Computing coefficients via DCT-II","text":"<p>PyChebyshev uses Type I Chebyshev nodes (roots of \\(T_n\\), also called Gauss--Chebyshev nodes):</p> \\[x_i = \\cos\\!\\left(\\frac{(2i - 1)\\,\\pi}{2n}\\right), \\quad i = 1, \\ldots, n\\] <p>For values sampled at these \\(n\\) nodes, the Chebyshev expansion coefficients \\(c_k\\) can be recovered exactly via the Discrete Cosine Transform (DCT-II).</p> <p>Why DCT-II works. The connection comes from the orthogonality of Chebyshev polynomials. Evaluating \\(T_k(x_i)\\) at the Type I nodes and exploiting the identity \\(T_k(\\cos\\theta) = \\cos(k\\theta)\\) turns the coefficient formula into a discrete cosine sum -- precisely the DCT-II. The computation runs in \\(O(n \\log n)\\) via FFT.</p> <p>In practice, the implementation:</p> <ol> <li>Reverses the node-order values (<code>::-1</code>) because <code>scipy.fft.dct</code> expects a    specific ordering convention, while PyChebyshev stores nodes in ascending order.</li> <li>Divides by \\(n\\) (the normalization factor for the DCT-II).</li> <li>Halves the zeroth coefficient (\\(c_0 \\mathrel{/}= 2\\)) because the Chebyshev    series convention uses \\(\\frac{c_0}{2} T_0(x) + c_1 T_1(x) + \\cdots\\), while the    raw DCT includes the full \\(c_0\\).</li> </ol>"},{"location":"user-guide/error-estimation/#reference","title":"Reference","text":"<p>This error estimation approach follows the ex ante method described in Ruiz &amp; Zeron, Machine Learning for Risk Calculations, Wiley Finance, 2021, Section 3.4.</p>"},{"location":"user-guide/error-estimation/#multi-dimensional-error","title":"Multi-Dimensional Error","text":"<p>For a \\(D\\)-dimensional interpolant on a tensor grid with \\(n_d\\) nodes in dimension \\(d\\), the error estimate generalizes as follows:</p> <ol> <li>Extract 1-D slices. For each dimension \\(d\\), fix all other indices and extract    every 1-D slice of the tensor along dimension \\(d\\).</li> <li>Compute per-slice error. For each slice, compute the Chebyshev coefficients    via DCT-II and take the magnitude of the last coefficient \\(|c_{n_d - 1}|\\).</li> <li>Maximize over slices. Take the maximum \\(|c_{n_d - 1}|\\) across all slices for    dimension \\(d\\). This worst-case slice represents the hardest-to-approximate 1-D    cross-section of the function along that axis.</li> <li>Sum across dimensions. The total error estimate is the sum of per-dimension    maxima:</li> </ol> \\[\\hat{E} = \\sum_{d=1}^{D} \\max_{\\text{slices along } d} |c_{n_d - 1}|\\] <p>Why sum across dimensions? PyChebyshev evaluates a multi-dimensional interpolant via dimensional decomposition -- contracting one axis at a time (see Multi-Dimensional Extension). At each contraction step, the 1-D interpolation error for that dimension is injected into the remaining computation. In the worst case, these per-dimension errors add up. The summation therefore represents a conservative heuristic: the total error is at most the sum of the worst per-dimension errors, assuming the errors do not cancel.</p> <p>Not a tight bound</p> <p>In practice, \\(\\hat{E}\\) is often pessimistic because: (a) the worst-case slice rarely coincides across all other index combinations, and (b) errors from different dimensions tend to partially cancel rather than align. The estimate is best used as an order-of-magnitude indicator, not as a precise bound.</p>"},{"location":"user-guide/error-estimation/#slider-error-estimation","title":"Slider Error Estimation","text":"<p><code>ChebyshevSlider</code> also supports <code>error_estimate()</code>. The slider error is the sum of the error estimates from each individual slide:</p> <pre><code>import math\nfrom pychebyshev import ChebyshevSlider\n\ndef f(x, _):\n    return math.sin(x[0]) + math.sin(x[1]) + math.sin(x[2])\n\nslider = ChebyshevSlider(\n    function=f,\n    num_dimensions=3,\n    domain=[[-1, 1], [-1, 1], [-1, 1]],\n    n_nodes=[11, 11, 11],\n    partition=[[0], [1], [2]],\n    pivot_point=[0.0, 0.0, 0.0],\n)\nslider.build()\nprint(f\"Slider error estimate: {slider.error_estimate():.2e}\")\n</code></pre> <p>Cross-group interaction error</p> <p>The slider error estimate captures per-slide interpolation error only -- the error from approximating each slide's low-dimensional function with Chebyshev polynomials. Error from the additive sliding decomposition itself (i.e., the cross-group coupling that the sliding formula ignores) is not included. For example, if \\(f(x_1, x_2) = x_1 \\cdot x_2\\) and the partition is <code>[[0], [1]]</code>, the sliding approximation is structurally unable to represent the product term, regardless of node count. The error estimate will report near-zero (each 1-D slide is well-resolved), but the true error can be large. For strongly coupled functions, always validate with test points.</p>"},{"location":"user-guide/error-estimation/#tensor-train-error-estimation","title":"Tensor Train Error Estimation","text":"<p><code>ChebyshevTT</code> also supports <code>error_estimate()</code>. The algorithm is analogous to the full-tensor case: for each core, convert to Chebyshev coefficients via DCT-II, take the maximum absolute value of the last coefficient slice (across all rank indices), and sum across dimensions.</p> <pre><code>from pychebyshev import ChebyshevTT\n\ntt = ChebyshevTT(my_func, 5, domain, [11]*5, max_rank=10)\ntt.build()\nprint(f\"TT error estimate: {tt.error_estimate():.2e}\")\n</code></pre> <p>Two sources of TT error</p> <p>The TT error estimate captures per-core coefficient truncation -- the error from using finitely many Chebyshev nodes within each core. It does not capture rank truncation error -- the error from representing the function with a low-rank TT decomposition. Think of it this way: even if every core perfectly resolves its 1-D slices (coefficient error \\(\\approx 0\\)), the TT may still be inaccurate because the rank is too low to capture all the coupling between dimensions.</p> <p>In practice, the rank truncation error dominates at low ranks (<code>max_rank</code> \\(\\leq 5\\)), while coefficient truncation dominates at high ranks with few nodes. The estimate is most reliable when <code>max_rank</code> is large enough that increasing it no longer improves accuracy.</p>"},{"location":"user-guide/error-estimation/#convergence-example","title":"Convergence Example","text":"<p>As the number of nodes increases, the error estimate should decrease -- rapidly for smooth functions. This example demonstrates spectral convergence for a 1-D function:</p> <pre><code>from pychebyshev import ChebyshevApproximation\nimport math\n\ndef f(x, _):\n    return math.sin(x[0])\n\nfor n in [5, 10, 15, 20, 25, 30]:\n    cheb = ChebyshevApproximation(f, 1, [[-1, 1]], [n])\n    cheb.build(verbose=False)\n    print(f\"n={n:2d}: error_estimate = {cheb.error_estimate():.2e}\")\n</code></pre> <p>For \\(\\sin(x)\\), which is entire (analytic everywhere in the complex plane), the coefficients decay exponentially, so you should see the estimate drop by several orders of magnitude as \\(n\\) grows.</p>"},{"location":"user-guide/error-estimation/#api-reference","title":"API Reference","text":"<p>For full method signatures and parameter details, see:</p> <ul> <li><code>ChebyshevApproximation.error_estimate()</code> -- error estimate for standard tensor interpolation.</li> <li><code>ChebyshevSlider.error_estimate()</code> -- error estimate for sliding approximation (sum of per-slide errors).</li> <li><code>ChebyshevTT.error_estimate()</code> -- error estimate for Tensor Train approximation (from coefficient cores).</li> </ul>"},{"location":"user-guide/greeks/","title":"Computing Greeks","text":"<p>PyChebyshev computes option Greeks (and any partial derivatives) analytically using spectral differentiation matrices \u2014 no finite differences needed.</p>"},{"location":"user-guide/greeks/#derivative-specification","title":"Derivative Specification","text":"<p>Derivatives are specified as a list of integers, one per dimension. Each integer is the derivative order with respect to that dimension.</p> <p>For a 5D function \\(V(S, K, T, \\sigma, r)\\):</p> Greek <code>derivative_order</code> Mathematical Price <code>[0, 0, 0, 0, 0]</code> \\(V\\) Delta <code>[1, 0, 0, 0, 0]</code> \\(\\partial V / \\partial S\\) Gamma <code>[2, 0, 0, 0, 0]</code> \\(\\partial^2 V / \\partial S^2\\) Vega <code>[0, 0, 0, 1, 0]</code> \\(\\partial V / \\partial \\sigma\\) Rho <code>[0, 0, 0, 0, 1]</code> \\(\\partial V / \\partial r\\)"},{"location":"user-guide/greeks/#example-black-scholes-greeks","title":"Example: Black-Scholes Greeks","text":"<pre><code>from pychebyshev import ChebyshevApproximation\n\ndef black_scholes_call(x, _):\n    S, K, T, sigma, r = x\n    # ... your pricing function here\n    return price\n\ncheb = ChebyshevApproximation(\n    black_scholes_call, 5,\n    domain=[[80, 120], [90, 110], [0.25, 1.0], [0.15, 0.35], [0.01, 0.08]],\n    n_nodes=[11, 11, 11, 11, 11],\n)\ncheb.build()\n\npoint = [100, 100, 1.0, 0.25, 0.05]\n\n# All Greeks at once (most efficient)\nprice, delta, gamma, vega, rho = cheb.vectorized_eval_multi(point, [\n    [0, 0, 0, 0, 0],\n    [1, 0, 0, 0, 0],\n    [2, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 1],\n])\n</code></pre>"},{"location":"user-guide/greeks/#how-it-works","title":"How It Works","text":"<ol> <li>At build time: Pre-compute differentiation matrix \\(D\\) from node positions</li> <li>At query time: Apply \\(D\\) to the function value tensor before barycentric interpolation</li> <li>For second derivatives: apply \\(D\\) twice (\\(D^2 \\mathbf{f}\\))</li> <li>Interpolate the resulting derivative values to the query point</li> </ol> <p>This provides exact derivatives of the interpolating polynomial. Because the differentiation matrix computes derivatives of the degree-\\(n\\) polynomial \\(p(x)\\) exactly (within machine precision), and \\(p(x)\\) converges spectrally to \\(f(x)\\), the derivative \\(p'(x)\\) also converges spectrally to \\(f'(x)\\).</p> <p>See Berrut &amp; Trefethen (2004) for the derivation of the differentiation matrix formulas. For the full theory of spectral differentiation, see Trefethen, Approximation Theory and Approximation Practice, SIAM 2019, Chapter 11.</p> <p>Tensor Train derivatives</p> <p><code>ChebyshevTT</code> uses finite differences instead of analytical derivatives, because the spectral differentiation matrix requires the full tensor (which TT avoids storing). FD derivatives are still accurate to within a few hundredths of a percent for first derivatives. See Tensor Train: Derivatives.</p>"},{"location":"user-guide/greeks/#accuracy","title":"Accuracy","text":"<p>With 11 nodes per dimension on a 5D Black-Scholes test:</p> Greek Max Error Delta &lt; 0.01% Gamma &lt; 0.01% Vega ~1.98% Rho &lt; 0.01% <p>Vega has slightly higher error because volatility sensitivity involves a product of multiple terms, but remains well within practical tolerance.</p>"},{"location":"user-guide/serialization/","title":"Saving &amp; Loading Interpolants","text":""},{"location":"user-guide/serialization/#why-save","title":"Why Save?","text":"<p>Building a Chebyshev interpolant is the expensive step \u2014 it evaluates your function at every node in the tensor grid (e.g. \\(11^5 = 161{,}051\\) evaluations for a 5-D problem). Once built, evaluation takes microseconds.</p> <p>Saving a built interpolant lets you:</p> <ul> <li>Build once, evaluate forever \u2014 skip the build step in production</li> <li>Share models \u2014 distribute pre-built interpolants to team members or across machines</li> <li>Persist across sessions \u2014 save your work and reload it later</li> </ul>"},{"location":"user-guide/serialization/#saving-a-built-interpolant","title":"Saving a Built Interpolant","text":"<p>All three public classes -- <code>ChebyshevApproximation</code>, <code>ChebyshevSlider</code>, and <code>ChebyshevTT</code> -- provide <code>save()</code> and <code>load()</code> methods:</p> <pre><code>import math\nfrom pychebyshev import ChebyshevApproximation\n\ndef my_func(x, _):\n    return math.sin(x[0]) * math.exp(-x[1])\n\ncheb = ChebyshevApproximation(\n    my_func, 2, [[-1, 1], [0, 2]], [15, 15]\n)\ncheb.build()\n\n# Save to disk\ncheb.save(\"interpolant.pkl\")\n</code></pre> <p>For a <code>ChebyshevSlider</code>:</p> <pre><code>from pychebyshev import ChebyshevSlider\n\nslider = ChebyshevSlider(\n    my_func, 2, [[-1, 1], [0, 2]], [15, 15],\n    partition=[[0], [1]],\n    pivot_point=[0.0, 1.0],\n)\nslider.build()\n\nslider.save(\"slider.pkl\")\n</code></pre> <p>For a <code>ChebyshevTT</code>:</p> <pre><code>from pychebyshev import ChebyshevTT\n\ntt = ChebyshevTT(my_func, 5, domain, [11]*5, max_rank=10)\ntt.build()\ntt.save(\"tt_model.pkl\")\n</code></pre>"},{"location":"user-guide/serialization/#loading-an-interpolant","title":"Loading an Interpolant","text":"<p>Use the <code>load()</code> class method \u2014 no rebuild needed:</p> <pre><code>from pychebyshev import ChebyshevApproximation, ChebyshevSlider, ChebyshevTT\n\n# Load and evaluate immediately\ncheb = ChebyshevApproximation.load(\"interpolant.pkl\")\nvalue = cheb.vectorized_eval([0.5, 1.0], [0, 0])\n\n# Works the same for sliders\nslider = ChebyshevSlider.load(\"slider.pkl\")\nvalue = slider.eval([0.5, 1.0], [0, 0])\n\n# And for Tensor Train\ntt = ChebyshevTT.load(\"tt_model.pkl\")\nvalue = tt.eval([0.5, 1.0, 0.3, 0.2, 0.05])\n</code></pre> <p>The loaded <code>ChebyshevApproximation</code> supports all evaluation methods (<code>vectorized_eval</code>, <code>vectorized_eval_multi</code>, <code>vectorized_eval_batch</code>). The loaded <code>ChebyshevSlider</code> supports <code>eval</code> and <code>eval_multi</code>. The loaded <code>ChebyshevTT</code> supports <code>eval</code>, <code>eval_batch</code>, and <code>eval_multi</code>.</p>"},{"location":"user-guide/serialization/#inspecting-objects","title":"Inspecting Objects","text":"<p>Use <code>repr()</code> for a compact summary and <code>print()</code> for a detailed view:</p> <pre><code>cheb = ChebyshevApproximation.load(\"interpolant.pkl\")\n\nrepr(cheb)\n# ChebyshevApproximation(dims=2, nodes=[15, 15], built=True)\n\nprint(cheb)\n# ChebyshevApproximation (2D, built)\n#   Nodes:       [15, 15] (225 total)\n#   Domain:      [-1, 1] x [0, 2]\n#   Build:       0.002s, 225 evaluations\n#   Derivatives: up to order 2\n</code></pre> <p>For a slider:</p> <pre><code>print(slider)\n# ChebyshevSlider (5D, 2 slides, built)\n#   Partition: [[0, 1, 2], [3, 4]]\n#   Pivot:     [100.0, 1.0, 0.25, 0.05, 0.2]\n#   Nodes:     [11, 11, 11, 11, 11] (1,452 vs 161,051 full tensor)\n#   Domain:    [80.0, 120.0] x [0.5, 2.0] x [0.01, 0.5] x [0.01, 0.1] x [0.05, 0.5]\n#   Slides:\n#     [0] dims [0, 1, 2]: 1,331 evals, built in 0.189s\n#     [1] dims [3, 4]:     121 evals, built in 0.021s\n</code></pre> <p>This is useful for verifying that a loaded interpolant matches your expectations before using it.</p>"},{"location":"user-guide/serialization/#limitations","title":"Limitations","text":"<ul> <li> <p>The original function is not saved. Only the numerical data needed for   evaluation (nodes, weights, tensor values, differentiation matrices) is persisted.   After loading, <code>obj.function</code> is <code>None</code>.</p> </li> <li> <p>Calling <code>build()</code> on a loaded object requires reassigning a function first:</p> <pre><code>cheb = ChebyshevApproximation.load(\"interpolant.pkl\")\ncheb.function = my_func  # reassign before rebuilding\ncheb.build()\n</code></pre> </li> <li> <p>Version compatibility. If you load a file saved with a different version of   PyChebyshev, a warning is emitted. Evaluation results should be identical unless   internal data layout changed between versions.</p> </li> </ul> <p>Security</p> <p><code>load()</code> uses Python's <code>pickle</code> module internally. Pickle can execute arbitrary code during deserialization. Only load files you trust. Do not load interpolants from untrusted or unverified sources.</p>"},{"location":"user-guide/sliding/","title":"Sliding Technique","text":"<p>The Sliding Technique enables Chebyshev approximation of high-dimensional functions by decomposing them into a sum of low-dimensional interpolants. This sidesteps the curse of dimensionality at the cost of losing cross-group interactions.</p>"},{"location":"user-guide/sliding/#motivation","title":"Motivation","text":"<p>A full tensor Chebyshev interpolant on \\(n\\) dimensions with \\(m\\) nodes per dimension requires \\(m^n\\) function evaluations. For \\(n = 10\\) and \\(m = 11\\), that is over 25 billion evaluations \u2014 clearly infeasible.</p> <p>The sliding technique partitions the dimensions into small groups and builds a separate Chebyshev interpolant (a slide) for each group, with all other dimensions fixed at a pivot point. The total cost becomes the sum of the group grid sizes rather than their product.</p>"},{"location":"user-guide/sliding/#algorithm","title":"Algorithm","text":"<p>Given \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), a pivot point \\(\\mathbf{z} = (z_1, \\ldots, z_n)\\), and a partition of dimensions into \\(k\\) groups:</p> <ol> <li>Evaluate the pivot value \\(v = f(\\mathbf{z})\\).</li> <li>For each group \\(i\\), build a slide \\(s_i\\) \u2014 a Chebyshev interpolant on the group's dimensions, with all other dimensions fixed at their pivot values.</li> <li>Evaluate using the additive formula:</li> </ol> \\[ f(\\mathbf{x}) \\approx v + \\sum_{i=1}^{k} \\bigl[ s_i(\\mathbf{x}_{G_i}) - v \\bigr] \\] <p>where \\(\\mathbf{x}_{G_i}\\) denotes the components of \\(\\mathbf{x}\\) belonging to group \\(i\\).</p>"},{"location":"user-guide/sliding/#when-to-use-sliding","title":"When to Use Sliding","text":"<p>Sliding works well when:</p> <ul> <li>The function is additively separable or nearly so (e.g., \\(\\sin(x_1) + \\sin(x_2) + \\sin(x_3)\\)).</li> <li>Cross-group interactions are weak relative to within-group effects.</li> <li>The number of dimensions is too large for full tensor interpolation (say, \\(n &gt; 6\\)).</li> </ul> <p>Sliding does not work well when:</p> <ul> <li>Variables in different groups are strongly coupled (e.g., Black-Scholes where \\(S\\), \\(T\\), and \\(\\sigma\\) interact multiplicatively).</li> <li>High accuracy is required far from the pivot point.</li> </ul> <p>Alternative: Tensor Train</p> <p>For general (non-separable) high-dimensional functions, consider <code>ChebyshevTT</code> instead. TT-Cross captures cross-variable coupling that the sliding decomposition misses, at the cost of using finite differences for derivatives instead of analytical spectral differentiation.</p> <p>Choosing the partition</p> <p>Group variables that have strong non-linear interactions together. For example, if \\(f = x_1^3 x_2^2 + x_3\\), group \\((x_1, x_2)\\) in one slide and \\(x_3\\) in another.</p>"},{"location":"user-guide/sliding/#usage","title":"Usage","text":"<pre><code>import math\nfrom pychebyshev import ChebyshevSlider\n\n# Additively separable function\ndef f(x, _):\n    return math.sin(x[0]) + math.sin(x[1]) + math.sin(x[2])\n\nslider = ChebyshevSlider(\n    function=f,\n    num_dimensions=3,\n    domain=[[-1, 1], [-1, 1], [-1, 1]],\n    n_nodes=[11, 11, 11],\n    partition=[[0], [1], [2]],       # each dim is its own slide\n    pivot_point=[0.0, 0.0, 0.0],\n)\nslider.build()\n\n# Evaluate function value\nval = slider.eval([0.5, 0.3, -0.2], [0, 0, 0])\n\n# Evaluate derivative w.r.t. x0\ndfdx0 = slider.eval([0.5, 0.3, -0.2], [1, 0, 0])\n</code></pre>"},{"location":"user-guide/sliding/#multi-dimensional-slides","title":"Multi-dimensional slides","text":"<p>For functions with within-group coupling, use larger groups:</p> <pre><code>def g(x, _):\n    return x[0]**3 * x[1]**2 + math.sin(x[2]) + math.sin(x[3])\n\nslider = ChebyshevSlider(\n    function=g,\n    num_dimensions=4,\n    domain=[[-2, 2], [-2, 2], [-1, 1], [-1, 1]],\n    n_nodes=[12, 12, 8, 8],\n    partition=[[0, 1], [2], [3]],    # 2D + 1D + 1D\n    pivot_point=[0.0, 0.0, 0.0, 0.0],\n)\nslider.build()\n</code></pre>"},{"location":"user-guide/sliding/#build-cost-comparison","title":"Build cost comparison","text":"<pre><code># Full tensor: 12 * 12 * 8 * 8 = 9,216 evaluations\n# Sliding:     12*12 + 8 + 8   = 160 evaluations  (57x fewer)\nprint(f\"Slider build evaluations: {slider.total_build_evals}\")\n</code></pre>"},{"location":"user-guide/sliding/#derivatives","title":"Derivatives","text":"<p>The slider supports analytical derivatives through its slides. Only the slide containing the differentiated dimension contributes:</p> \\[ \\frac{\\partial}{\\partial x_j} f(\\mathbf{x}) \\approx \\frac{\\partial}{\\partial x_j} s_i(\\mathbf{x}_{G_i}) \\] <p>where \\(j \\in G_i\\). The pivot value \\(v\\) is constant and drops out.</p> <pre><code># Multiple derivatives at once\nresults = slider.eval_multi(\n    [0.5, 0.3, -0.2],\n    [\n        [0, 0, 0],  # function value\n        [1, 0, 0],  # d/dx0\n        [0, 1, 0],  # d/dx1\n        [0, 0, 1],  # d/dx2\n    ],\n)\n</code></pre>"},{"location":"user-guide/sliding/#limitations","title":"Limitations","text":""},{"location":"user-guide/sliding/#cross-group-derivatives-are-zero","title":"Cross-group derivatives are zero","text":"<p>Because slides are independent functions of disjoint variable groups, mixed partial derivatives across groups are exactly zero. For example, with partition <code>[[0, 1], [2]]</code>:</p> <ul> <li>\\(\\frac{\\partial^2 f}{\\partial x_0 \\partial x_1}\\) \u2014 computed within the <code>[0, 1]</code> slide (correct)</li> <li>\\(\\frac{\\partial^2 f}{\\partial x_0 \\partial x_2}\\) \u2014 returns 0 (x\u2080 and x\u2082 are in different slides)</li> </ul> <p>This is mathematically correct for the sliding approximation, but may differ from the true function's cross-derivatives. If cross-group sensitivities matter, group those variables together or use full tensor interpolation.</p>"},{"location":"user-guide/sliding/#accuracy-degrades-far-from-pivot","title":"Accuracy degrades far from pivot","text":"<p>The sliding approximation is most accurate near the pivot point. As the evaluation point moves away from the pivot in multiple dimensions simultaneously, cross-coupling errors accumulate. For strongly coupled functions like Black-Scholes, this can produce 20-50% errors at domain boundaries.</p>"},{"location":"user-guide/sliding/#api-reference","title":"API Reference","text":"<p>Chebyshev Sliding approximation for high-dimensional functions.</p> <p>Decomposes f(x_1, ..., x_n) into a sum of low-dimensional Chebyshev interpolants (slides) around a pivot point z:</p> <pre><code>f(x) \u2248 f(z) + \u03a3_i [s_i(x_group_i) - f(z)]\n</code></pre> <p>where each slide s_i is a ChebyshevApproximation built on a subset of dimensions with the remaining dimensions fixed at z.</p> <p>This trades accuracy for dramatically reduced build cost: instead of evaluating f at n_1 \u00d7 n_2 \u00d7 ... \u00d7 n_d grid points (exponential), the slider evaluates at n_1 \u00d7 n_2 + n_3 \u00d7 n_4 + ... (sum of products within each group).</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>callable</code> <p>Function to approximate. Signature: <code>f(point, data) -&gt; float</code> where <code>point</code> is a list of floats and <code>data</code> is arbitrary additional data (can be None).</p> required <code>num_dimensions</code> <code>int</code> <p>Total number of input dimensions.</p> required <code>domain</code> <code>list of (float, float)</code> <p>Bounds [lo, hi] for each dimension.</p> required <code>n_nodes</code> <code>list of int</code> <p>Number of Chebyshev nodes per dimension.</p> required <code>partition</code> <code>list of list of int</code> <p>Grouping of dimension indices into slides. Each dimension must appear in exactly one group. E.g. <code>[[0,1,2], [3,4]]</code> creates a 3D slide for dims 0,1,2 and a 2D slide for dims 3,4.</p> required <code>pivot_point</code> <code>list of float</code> <p>Reference point z around which slides are built.</p> required <code>max_derivative_order</code> <code>int</code> <p>Maximum derivative order to pre-compute (default 2).</p> <code>2</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; def f(x, _):\n...     return math.sin(x[0]) + math.sin(x[1]) + math.sin(x[2])\n&gt;&gt;&gt; slider = ChebyshevSlider(\n...     f, 3, [[-1,1], [-1,1], [-1,1]], [11,11,11],\n...     partition=[[0], [1], [2]],\n...     pivot_point=[0.0, 0.0, 0.0],\n... )\n&gt;&gt;&gt; slider.build(verbose=False)\n&gt;&gt;&gt; round(slider.eval([0.5, 0.3, 0.1], [0,0,0]), 4)\n0.8764\n</code></pre>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.total_build_evals","title":"<code>total_build_evals</code>  <code>property</code>","text":"<p>Total number of function evaluations used during build.</p>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.build","title":"<code>build(verbose=True)</code>","text":"<p>Build all slides by evaluating the function at slide-specific grids.</p> <p>For each slide, dimensions outside the slide group are fixed at their pivot values.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print build progress. Default is True.</p> <code>True</code>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.eval","title":"<code>eval(point, derivative_order)</code>","text":"<p>Evaluate the slider approximation at a point.</p> <p>Uses Equation 7.5 from Ruiz &amp; Zeron (2021):     f(x) \u2248 f(z) + \u03a3_i [s_i(x_i) - f(z)]</p> <p>For derivatives, only the slide containing that dimension contributes.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Evaluation point in the full n-dimensional space.</p> required <code>derivative_order</code> <code>list of int</code> <p>Derivative order for each dimension (0 = function value).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Approximated function value or derivative.</p>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.eval_multi","title":"<code>eval_multi(point, derivative_orders)</code>","text":"<p>Evaluate slider at multiple derivative orders for the same point.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Evaluation point in the full n-dimensional space.</p> required <code>derivative_orders</code> <code>list of list of int</code> <p>Each inner list specifies derivative order per dimension.</p> required <p>Returns:</p> Type Description <code>list of float</code> <p>Results for each derivative order.</p>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.error_estimate","title":"<code>error_estimate()</code>","text":"<p>Estimate the sliding approximation error.</p> <p>Returns the sum of per-slide Chebyshev error estimates. Each slide's error is estimated independently using the Chebyshev coefficient method from Ruiz &amp; Zeron (2021), Section 3.4.</p> <p>Note: This captures per-slide interpolation error only. Cross-group interaction error (inherent to the sliding decomposition) is not included.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated interpolation error (per-slide sum).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Return picklable state, excluding the original function.</p>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore state from a pickled dict.</p>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.save","title":"<code>save(path)</code>","text":"<p>Save the built slider to a file.</p> <p>The original function is not saved \u2014 only the numerical data needed for evaluation. The saved file can be loaded with :meth:<code>load</code> without access to the original function.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Destination file path.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the slider has not been built yet.</p>"},{"location":"user-guide/sliding/#pychebyshev.slider.ChebyshevSlider.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load a previously saved slider from a file.</p> <p>The loaded object can evaluate immediately; no rebuild is needed. The <code>function</code> attribute will be <code>None</code>. Assign a new function before calling <code>build()</code> again if a rebuild is desired.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Path to the saved file.</p> required <p>Returns:</p> Type Description <code>ChebyshevSlider</code> <p>The restored slider.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the file was saved with a different PyChebyshev version.</p> <code>.. warning::</code> <p>This method uses :mod:<code>pickle</code> internally. Pickle can execute arbitrary code during deserialization. Only load files you trust.</p>"},{"location":"user-guide/tensor-train/","title":"Tensor Train Interpolation","text":""},{"location":"user-guide/tensor-train/#introduction","title":"Introduction","text":"<p>The Tensor Train (TT) format enables Chebyshev interpolation of functions with 5 or more dimensions by decomposing the full coefficient tensor into a chain of small 3D cores. Instead of storing and building the full \\(n^d\\) tensor (infeasible for \\(d \\geq 6\\)), TT stores \\(O(d \\cdot n \\cdot r^2)\\) elements, where \\(r\\) is the TT rank -- a measure of the function's internal complexity.</p>"},{"location":"user-guide/tensor-train/#when-to-use-which-class","title":"When to use which class","text":"Scenario Class Why \\(d \\leq 5\\) <code>ChebyshevApproximation</code> Full tensor is feasible; analytical derivatives \\(d \\geq 5\\), general function <code>ChebyshevTT</code> TT-Cross builds from \\(O(d \\cdot n \\cdot r^2)\\) evaluations \\(d \\gg 5\\), separable function <code>ChebyshevSlider</code> Additive decomposition; cheapest build, but loses cross-group coupling <p><code>ChebyshevTT</code> fills the gap between the full tensor approach (limited to low dimensions) and the sliding technique (limited to separable functions). It handles general functions -- including those with strong multiplicative coupling like Black-Scholes -- at a fraction of the full tensor cost.</p>"},{"location":"user-guide/tensor-train/#quick-start","title":"Quick Start","text":"<pre><code>import math\nfrom scipy.stats import norm\nfrom pychebyshev import ChebyshevTT\n\ndef black_scholes_5d(x, _):\n    \"\"\"European call price: V(S, K, T, sigma, r).\"\"\"\n    S, K, T, sigma, r = x\n    d1 = (math.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * math.sqrt(T))\n    d2 = d1 - sigma * math.sqrt(T)\n    return S * norm.cdf(d1) - K * math.exp(-r * T) * norm.cdf(d2)\n\ntt = ChebyshevTT(\n    black_scholes_5d, 5,\n    domain=[[80, 120], [90, 110], [0.25, 1.0], [0.15, 0.35], [0.01, 0.08]],\n    n_nodes=[11, 11, 11, 11, 11],\n    max_rank=15,\n)\ntt.build()\nprice = tt.eval([100, 100, 1.0, 0.25, 0.05])\nprint(f\"TT price: {price:.6f}\")\n</code></pre> <p>After building, <code>print(tt)</code> shows a summary including TT ranks and compression ratio:</p> <pre><code>ChebyshevTT (5D, built)\n  Nodes:       [11, 11, 11, 11, 11]\n  TT ranks:    [1, 11, 11, 11, 7, 1]\n  Compression: 161,051 -&gt; 3,707 elements (43.4x)\n  Build:       0.351s (7,419 function evals)\n  Domain:      [80, 120] x [90, 110] x [0.25, 1.0] x [0.15, 0.35] x [0.01, 0.08]\n  Error est:   1.23e-06\n</code></pre>"},{"location":"user-guide/tensor-train/#how-it-works","title":"How It Works","text":""},{"location":"user-guide/tensor-train/#tt-format","title":"TT format","text":"<p>A \\(d\\)-dimensional tensor \\(\\mathcal{A}\\) with indices \\(j_1, \\ldots, j_d\\) is represented as a chain of 3D cores:</p> \\[\\mathcal{A}(j_1, \\ldots, j_d) = G_1(j_1) \\cdot G_2(j_2) \\cdots G_d(j_d)\\] <p>Each core \\(G_k\\) is a 3D array of shape \\((r_{k-1}, n_k, r_k)\\), and for a fixed index \\(j_k\\) the slice \\(G_k(j_k)\\) is an \\(r_{k-1} \\times r_k\\) matrix. The product of these matrices yields the tensor element. The boundary ranks are \\(r_0 = r_d = 1\\), so the result is a scalar.</p> <p>Storage: The full tensor has \\(\\prod_k n_k\\) elements. The TT format stores \\(\\sum_k r_{k-1} \\cdot n_k \\cdot r_k\\) elements -- exponentially smaller when ranks are moderate.</p>"},{"location":"user-guide/tensor-train/#tt-cross-build-algorithm","title":"TT-Cross build algorithm","text":"<p>The key advantage of <code>ChebyshevTT</code> over full tensor interpolation is how it is built. Instead of evaluating the function at every node combination (\\(n^d\\) points), the TT-Cross algorithm (Oseledets &amp; Tyrtyshnikov 2010) evaluates at strategically selected grid points. For 5D with \\(n = 11\\) and <code>max_rank=15</code>:</p> <ul> <li>Full tensor: 161,051 evaluations</li> <li>TT-Cross: ~7,400 unique evaluations (21.7x fewer)</li> </ul> <p>The algorithm performs alternating sweeps:</p> <p>1. Initialize. Randomly select multi-index sets \\(J_{\\text{right}}[k]\\) for each dimension, each containing \\(r_k\\) multi-indices into the \"right\" dimensions \\(k+1, \\ldots, d-1\\).</p> <p>2. Left-to-right sweep. For each dimension \\(k = 0, \\ldots, d-2\\):</p> <ul> <li> <p>Build a cross matrix \\(C\\) of shape \\((r_{k-1} \\cdot n_k, r_k)\\).   Each row corresponds to a (left multi-index, node index) pair, and each   column to a right multi-index. Each entry is a function evaluation at   the combined grid point. This is the most expensive step -- it evaluates   the function at \\(r_{k-1} \\cdot n_k \\cdot r_k\\) grid points (with caching,   many of these are free on subsequent sweeps).</p> </li> <li> <p>SVD for rank selection. Compute \\(C = U \\Sigma V^T\\) and determine the   effective rank by counting singular values above a tight threshold   (\\(10^{-12} \\cdot \\sigma_{\\max}\\)). This is further capped by the per-mode   rank bound (see Optimizations below).</p> </li> <li> <p>Maxvol pivot selection. Apply the maxvol algorithm to the left   singular vectors \\(U\\) to select the \\(r\\) rows whose submatrix has   approximately maximal determinant. These pivots identify the most   \"informative\" (left, node) index pairs.</p> </li> <li> <p>Form the TT core via cross interpolation:   \\(\\hat{C} = U \\cdot U[\\text{pivots}]^{-1}\\), then reshape to   \\((r_{k-1}, n_k, r_k)\\). The identity \\(\\hat{C}[\\text{pivots}] = I\\)   ensures exact interpolation at the selected cross points.</p> </li> <li> <p>Update left index set for dimension \\(k+1\\) by expanding each   pivot into its constituent (left multi-index, node index) pair.</p> </li> </ul> <p>3. Convergence check. After completing the L\\(\\to\\)R sweep, evaluate the TT at 20 random grid points and compare against exact function values. If the relative error is below <code>tolerance</code>, stop immediately (skipping the R\\(\\to\\)L sweep).</p> <p>4. Right-to-left sweep. Analogous to L\\(\\to\\)R but processes dimensions from \\(d-1\\) down to 1. The transposed cross matrix \\(C^T\\) is used, and the maxvol pivots update the right index sets.</p> <p>5. Convergence and plateau check. After the R\\(\\to\\)L sweep, evaluate error again. The algorithm tracks the best cores seen across all checks (see Best-cores tracking) and stops if no improvement has been observed for 3 consecutive checks.</p> <p>6. Repeat sweeps until converged, stale, or <code>max_sweeps</code> is reached.</p>"},{"location":"user-guide/tensor-train/#maxvol-algorithm","title":"Maxvol algorithm","text":"<p>The maximum-volume (maxvol) algorithm is a key subroutine within TT-Cross. Given a tall matrix \\(A\\) of shape \\((m, r)\\) with \\(m \\geq r\\), it finds \\(r\\) row indices such that the submatrix \\(A[\\text{idx}]\\) has approximately maximal \\(|\\det|\\).</p> <p>Why maximum volume? The cross interpolation formula \\(\\hat{C} = A \\cdot A[\\text{idx}]^{-1}\\) requires inverting the \\(r \\times r\\) submatrix \\(A[\\text{idx}]\\). A near-singular submatrix (small determinant) would amplify errors in this inversion, producing inaccurate TT cores. Maximizing \\(|\\det(A[\\text{idx}])|\\) is equivalent to selecting the \\(r\\) most linearly independent rows of \\(A\\) -- this minimizes the condition number of the inversion and ensures numerically stable cross interpolation.</p> <p>In the context of TT-Cross, each row of \\(A\\) corresponds to a particular grid point (a combination of left multi-index and Chebyshev node). Maxvol therefore selects the grid points that carry the most \"information\" about the function, avoiding redundant or nearly-collinear samples.</p> <p>The implementation uses two phases:</p> <ol> <li> <p>Initialization via column-pivoted QR. Compute    <code>Q, R, piv = qr(A.T, pivoting=True)</code>. The first \\(r\\) pivot indices    identify the \\(r\\) most linearly independent rows of \\(A\\) -- a good    starting point.</p> </li> <li> <p>Iterative refinement. Compute the coefficient matrix    \\(B = A \\cdot A[\\text{idx}]^{-1}\\) (shape \\(m \\times r\\)). While    \\(\\max|B_{ij}| &gt; 1.05\\):</p> <ul> <li>Find the entry \\((i, j)\\) with largest \\(|B_{ij}|\\).</li> <li>Swap: replace row \\(j\\) in the index set with row \\(i\\).</li> <li>Rank-1 update of \\(B\\) (avoids re-inverting the full matrix).</li> </ul> </li> </ol> <p>Each swap strictly increases \\(|\\det(A[\\text{idx}])|\\), and the algorithm converges in \\(O(r^2)\\) iterations in practice.</p>"},{"location":"user-guide/tensor-train/#coefficient-conversion","title":"Coefficient conversion","text":"<p>After TT-Cross produces cores containing function values at Chebyshev nodes, a DCT-II is applied along the middle axis of each core to convert from function values to Chebyshev expansion coefficients:</p> <pre><code>from scipy.fft import dct\n\n# core shape: (r_{k-1}, n_k, r_k)\ncoeff_core = dct(core[:, ::-1, :], type=2, axis=1) / n_k\ncoeff_core[:, 0, :] /= 2\n</code></pre> <p>The reversal (<code>::-1</code>) converts from ascending to descending node order, which is the convention expected by the DCT-II. The division by \\(n_k\\) normalizes the transform, and halving the zeroth coefficient accounts for the Chebyshev series convention (\\(\\frac{c_0}{2} T_0 + c_1 T_1 + \\cdots\\)). See Error Estimation: Computing coefficients via DCT-II for the mathematical justification.</p>"},{"location":"user-guide/tensor-train/#evaluation-via-tt-inner-product","title":"Evaluation via TT inner product","text":"<p>Given the pre-computed coefficient cores \\(\\mathcal{C}\\) and a query point \\(p\\), evaluation computes the inner product:</p> \\[I(p) = \\langle \\mathcal{C},\\, \\mathcal{T}_p \\rangle\\] <p>where \\(\\mathcal{T}_p\\) is a rank-1 tensor whose entries are Chebyshev polynomial values \\([T_0(\\tilde{x}_k), T_1(\\tilde{x}_k), \\ldots, T_{n_k-1}(\\tilde{x}_k)]\\) at the scaled query coordinate \\(\\tilde{x}_k\\) in each dimension \\(k\\).</p> <p>In practice, this inner product is computed as a chain of matrix contractions:</p> <pre><code>result = np.ones((1, 1))\nfor k in range(num_dims):\n    q = chebyshev_polynomials(scaled_x[k], n_nodes[k])   # (n_k,)\n    v = np.einsum('j,ijk-&gt;ik', q, coeff_cores[k])        # (r_{k-1}, r_k)\n    result = result @ v\nvalue = result[0, 0]\n</code></pre> <p>Each step contracts one dimension, reducing the chain until a scalar remains. The cost is \\(O(d \\cdot n \\cdot r^2)\\) per point.</p>"},{"location":"user-guide/tensor-train/#optimizations","title":"Optimizations","text":"<p>PyChebyshev's TT-Cross implementation includes several optimizations that reduce function evaluations by 10--20x compared to a naive implementation.</p>"},{"location":"user-guide/tensor-train/#eval-caching","title":"Eval caching","text":"<p>Function evaluations are cached in a dictionary keyed by the grid index tuple. When the same grid point is needed again -- whether during the R\\(\\to\\)L sweep of the same iteration, or in subsequent sweeps -- the cached value is returned instantly. This is the single largest optimization: for 5D Black-Scholes with <code>max_rank=15</code>, caching reduces evaluations from ~85,000 to ~7,400.</p>"},{"location":"user-guide/tensor-train/#per-mode-rank-caps","title":"Per-mode rank caps","text":"<p>At bond \\(k\\), the theoretical maximum TT rank is \\(\\min(\\prod_{i&lt;k} n_i,\\; \\prod_{i \\geq k} n_i)\\). For example, with 5 dimensions of 11 nodes each, the first bond can have rank at most \\(\\min(11, 14641) = 11\\). PyChebyshev automatically caps the rank at each bond to this theoretical limit, preventing the algorithm from attempting ranks that are mathematically impossible.</p>"},{"location":"user-guide/tensor-train/#svd-based-adaptive-rank","title":"SVD-based adaptive rank","text":"<p>Instead of always using <code>max_rank</code> columns from a QR decomposition, the cross matrix is decomposed via SVD. Singular values below \\(10^{-12} \\cdot \\sigma_{\\max}\\) are dropped, so dimensions where the function has low effective rank naturally get smaller cores. For the 5D Black-Scholes example, this produces adaptive ranks <code>[1, 11, 11, 11, 7, 1]</code> instead of a uniform <code>[1, 11, 11, 11, 11, 1]</code> -- the last bond only needs rank 7 because the interest rate dimension has simpler structure.</p>"},{"location":"user-guide/tensor-train/#half-sweep-convergence","title":"Half-sweep convergence","text":"<p>Error is checked after the L\\(\\to\\)R half-sweep. If the TT already reproduces the function to within <code>tolerance</code> at random test points, the R\\(\\to\\)L sweep is skipped entirely. For separable functions like \\(\\sin(x) + \\sin(y) + \\sin(z)\\), this means convergence in a single L\\(\\to\\)R pass with only 159 evaluations.</p>"},{"location":"user-guide/tensor-train/#best-cores-tracking","title":"Best-cores tracking","text":"<p>TT-Cross error can oscillate between L\\(\\to\\)R and R\\(\\to\\)L sweeps -- a good L\\(\\to\\)R result may be partially degraded by R\\(\\to\\)L rebalancing. PyChebyshev keeps a copy of the best cores (lowest error) seen across all convergence checks, and returns those when the algorithm stops. This prevents the final result from being worse than an intermediate result.</p> <p>The algorithm also counts \"stale checks\" -- consecutive convergence checks that fail to improve the best error by at least 10%. After 3 stale checks (and best error below \\(10^{-3}\\)), the algorithm stops early, returning the best cores found.</p>"},{"location":"user-guide/tensor-train/#tt-svd-validation-build","title":"TT-SVD (validation build)","text":"<p>For moderate dimensions (\\(d \\leq 6\\)), the <code>method='svd'</code> option builds the full tensor and decomposes it via sequential truncated SVD. This produces optimal TT ranks (up to the SVD truncation tolerance) and is useful for:</p> <ul> <li>Validating TT-Cross accuracy against the best possible TT decomposition.</li> <li>Confirming that the function's intrinsic TT rank structure matches   expectations.</li> <li>Problems where the function is cheap to evaluate and a full tensor build   is acceptable.</li> </ul> <pre><code># TT-SVD \u2014 for validation or moderate dimensions\ntt.build(method=\"svd\")\n</code></pre> <p>In TT-SVD, singular values below <code>tolerance * sigma_max</code> are discarded at each unfolding. For a separable function like \\(\\sin(x) + \\sin(y) + \\sin(z)\\), TT-SVD finds exact rank <code>[1, 2, 2, 1]</code>.</p>"},{"location":"user-guide/tensor-train/#controlling-accuracy","title":"Controlling Accuracy","text":""},{"location":"user-guide/tensor-train/#max_rank","title":"<code>max_rank</code>","text":"<p>The TT rank controls how much \"coupling\" between dimensions the approximation can capture. Higher rank means more accurate, but more expensive to build and store.</p> <code>max_rank</code> Typical use 5--10 Smooth, nearly separable functions 10--15 General smooth functions (e.g., Black-Scholes) 20--30 Functions with strong nonlinear coupling <p>Start low, increase if needed</p> <p>Begin with <code>max_rank=10</code> and check <code>error_estimate()</code>. If the error is too large, increase to 15 or 20. Smooth functions like Black-Scholes options typically converge well with ranks of 5--15.</p> <p>The table below shows how <code>max_rank</code> affects the 5D Black-Scholes approximation (11 nodes per dimension, <code>seed=42</code>):</p> <code>max_rank</code> Unique evals Max price error TT ranks 8 ~4,500 0.58% [1, 8, 8, 8, 6, 1] 10 ~7,500 0.09% [1, 10, 10, 10, 7, 1] 15 ~7,400 0.19% [1, 11, 11, 11, 7, 1] <p>Note that <code>max_rank=15</code> and <code>max_rank=10</code> use a similar number of evaluations because per-mode rank caps limit interior ranks to at most \\(n = 11\\).</p>"},{"location":"user-guide/tensor-train/#tolerance","title":"<code>tolerance</code>","text":"<p>The convergence tolerance for TT-Cross. The algorithm checks relative error at random grid points after each half-sweep. When the error drops below this threshold, iteration stops. The default <code>1e-6</code> is appropriate for most problems.</p> <p>Tolerance does not control SVD truncation</p> <p>The <code>tolerance</code> parameter only controls when the sweep loop stops. Rank selection within each mode uses a fixed threshold of \\(10^{-12}\\) to drop only numerically zero singular values. Rank is primarily controlled by <code>max_rank</code> and the per-mode caps.</p>"},{"location":"user-guide/tensor-train/#max_sweeps","title":"<code>max_sweeps</code>","text":"<p>The maximum number of alternating left-right sweeps. The default of 10 is sufficient for most well-behaved functions. In practice, the best-cores tracking and stale-check stopping mean the algorithm often finishes in 2--3 sweeps.</p>"},{"location":"user-guide/tensor-train/#build-method","title":"Build method","text":"<p>The <code>build()</code> method accepts a <code>method</code> parameter:</p> <ul> <li><code>method='cross'</code> (default): TT-Cross algorithm -- evaluates only   \\(O(d \\cdot n \\cdot r^2)\\) points. Use for high-dimensional problems.</li> <li><code>method='svd'</code>: Builds the full tensor and decomposes via truncated SVD.   Only feasible for moderate dimensions (\\(d \\leq 6\\)), but produces optimal   ranks and is useful for validation.</li> </ul> <pre><code># TT-Cross (default) \u2014 efficient for high dimensions\ntt.build(method=\"cross\", seed=42)\n\n# TT-SVD \u2014 for validation or moderate dimensions\ntt.build(method=\"svd\")\n</code></pre>"},{"location":"user-guide/tensor-train/#error-estimation","title":"Error estimation","text":"<p>Like <code>ChebyshevApproximation</code>, the TT class supports ex ante error estimation from its coefficient cores:</p> <pre><code>tt.build()\nprint(f\"Estimated error: {tt.error_estimate():.2e}\")\n</code></pre> <p>Approximate for TT</p> <p>The TT error estimate is based on the trailing Chebyshev coefficients within each core. It captures per-core truncation error but does not account for rank truncation error. The true error may be somewhat larger than the estimate, especially at low ranks.</p>"},{"location":"user-guide/tensor-train/#derivatives-via-finite-differences","title":"Derivatives via Finite Differences","text":"<p>The TT format does not support analytical derivatives (the spectral differentiation matrix approach used by <code>ChebyshevApproximation</code> requires the full tensor). Instead, <code>ChebyshevTT</code> computes derivatives via central finite differences:</p> \\[\\frac{\\partial f}{\\partial x_k} \\approx \\frac{f(x + h\\, e_k) - f(x - h\\, e_k)}{2h}\\] <p>The <code>eval_multi()</code> method computes price and Greeks in a single call:</p> <pre><code>point = [100, 100, 1.0, 0.25, 0.05]\n\nresults = tt.eval_multi(point, [\n    [0, 0, 0, 0, 0],  # price\n    [1, 0, 0, 0, 0],  # Delta (dV/dS)\n    [2, 0, 0, 0, 0],  # Gamma (d\u00b2V/dS\u00b2)\n    [0, 0, 0, 1, 0],  # Vega  (dV/dsigma)\n    [0, 0, 0, 0, 1],  # Rho   (dV/dr)\n])\nprice, delta, gamma, vega, rho = results\n</code></pre> <p>The step size \\(h\\) is chosen automatically as \\(10^{-4}\\) times the domain width in each dimension. Points near domain boundaries are nudged inward to ensure the FD stencil stays inside the domain.</p> <p>FD accuracy</p> <p>For first-order derivatives, accuracy is typically within a few hundredths of a percent of the analytical value. Second-order derivatives (e.g., Gamma) are less precise due to the inherent amplification of noise in central second differences.</p>"},{"location":"user-guide/tensor-train/#batch-evaluation","title":"Batch Evaluation","text":"<p>For evaluating many points at once -- common in portfolio pricing -- use <code>eval_batch()</code>:</p> <pre><code>import numpy as np\n\n# 1000 random points in the domain\npoints = np.column_stack([\n    np.random.uniform(80, 120, 1000),    # S\n    np.random.uniform(90, 110, 1000),    # K\n    np.random.uniform(0.25, 1.0, 1000),  # T\n    np.random.uniform(0.15, 0.35, 1000), # sigma\n    np.random.uniform(0.01, 0.08, 1000), # r\n])\n\nprices = tt.eval_batch(points)  # (1000,) array\n</code></pre> <p><code>eval_batch()</code> vectorizes the TT inner product over all points simultaneously using <code>np.einsum</code> for batched matrix contractions, which is significantly faster than calling <code>eval()</code> in a loop. Typical speedup is 15--20x.</p>"},{"location":"user-guide/tensor-train/#performance-comparison","title":"Performance Comparison","text":""},{"location":"user-guide/tensor-train/#pychebyshev-vs-mocax-extend-tensor-train","title":"PyChebyshev vs MoCaX Extend (Tensor Train)","text":"<p>Both PyChebyshev <code>ChebyshevTT</code> and MoCaX <code>MocaxExtend</code> build Chebyshev interpolants in Tensor Train format. They differ in how the TT is constructed:</p> PyChebyshev <code>ChebyshevTT</code> MoCaX <code>MocaxExtend</code> Build algorithm TT-Cross (maxvol pivoting) Rank-adaptive ALS on random subgrid Point selection Adaptive via cross interpolation Random subset of full Chebyshev grid Eval implementation Vectorized NumPy (einsum, BLAS) Python loops + deep copies Coefficient cores Pre-computed via DCT-II Recomputed on every eval call <p>The following benchmarks are from 5D Black-Scholes \\(V(S, K, T, \\sigma, r)\\) with 11 Chebyshev nodes per dimension, dividend yield \\(q = 0.02\\).</p>"},{"location":"user-guide/tensor-train/#build-comparison","title":"Build comparison","text":"Metric PyChebyshev MoCaX Build time 0.35s 5.73s Function evaluations 7,419 8,000 <p>PyChebyshev uses slightly fewer evaluations (TT-Cross adaptively selects the most informative points) and builds 16x faster (no Python-level ALS optimization loop).</p>"},{"location":"user-guide/tensor-train/#accuracy-50-random-test-points","title":"Accuracy (50 random test points)","text":"Metric PyChebyshev MoCaX Mean price error 0.002% 0.093% Max price error 0.014% 0.712% Median price error 0.001% 0.045% <p>PyChebyshev is 40--50x more accurate at comparable evaluation budgets.</p>"},{"location":"user-guide/tensor-train/#evaluation-speed-1000-random-points","title":"Evaluation speed (1000 random points)","text":"Method PyChebyshev MoCaX Single eval 0.065 ms/query -- Batch eval 0.004 ms/query 0.246 ms/query <p>PyChebyshev batch evaluation is 58x faster than MoCaX.</p>"},{"location":"user-guide/tensor-train/#greeks-accuracy-10-scenarios-vs-analytical","title":"Greeks accuracy (10 scenarios vs analytical)","text":"Greek PyChebyshev avg error MoCaX avg error Delta 0.029% 0.379% Gamma 0.019% 1.604% <p>Both use central finite differences. PyChebyshev's advantage comes from its more accurate underlying interpolant.</p>"},{"location":"user-guide/tensor-train/#reproducing-the-comparison","title":"Reproducing the comparison","text":"<p>The comparison script <code>compare_tensor_train.py</code> in the repository root runs all the benchmarks above. It requires the MoCaX C++ library (not publicly available); PyChebyshev results are shown regardless.</p> <pre><code># Run the comparison (MoCaX portion is skipped if unavailable)\nuv run --with tqdm --with blackscholes python compare_tensor_train.py\n</code></pre> <p>If you have MoCaX installed, ensure the <code>mocaxextend_lib/</code> directory with <code>shared_libs/</code> (containing <code>libtensorvals.so</code> and <code>libhommat.so</code>) is in the repository root.</p> <p>MoCaX results are nondeterministic</p> <p>MoCaX uses a random subgrid for its ALS optimization, so its accuracy varies between runs. The numbers above are representative of typical runs.</p>"},{"location":"user-guide/tensor-train/#comparison-with-other-pychebyshev-methods","title":"Comparison with Other PyChebyshev Methods","text":"<code>ChebyshevApproximation</code> <code>ChebyshevTT</code> <code>ChebyshevSlider</code> Dimensions \\(\\leq 5\\) (practical) \\(5+\\) Any (with caveats) Build cost \\(O(n^d)\\) evaluations \\(O(d \\cdot n \\cdot r^2)\\) evaluations $\\sum_i O(n_{g_i}^{ Eval cost \\(O(d \\cdot n)\\) via BLAS GEMV \\(O(d \\cdot n \\cdot r^2)\\) via einsum \\(O(k \\cdot d_g \\cdot n)\\) per slide Derivatives Analytical (spectral) Finite differences Analytical (per-slide) Accuracy Spectral convergence Rank-dependent Depends on separability Best for Low-\\(d\\), high-accuracy Greeks Moderate-\\(d\\), general functions High-\\(d\\), separable functions"},{"location":"user-guide/tensor-train/#serialization","title":"Serialization","text":"<p><code>ChebyshevTT</code> supports saving and loading via pickle, following the same pattern as the other PyChebyshev classes:</p> <pre><code># Save after building\ntt.save(\"bs_5d_tt.pkl\")\n\n# Load later (no rebuild needed)\nfrom pychebyshev import ChebyshevTT\ntt_loaded = ChebyshevTT.load(\"bs_5d_tt.pkl\")\nprice = tt_loaded.eval([100, 100, 1.0, 0.25, 0.05])\n</code></pre> <p>Note</p> <p>The original function is not saved -- only the pre-computed coefficient cores and metadata. After loading, <code>eval()</code>, <code>eval_batch()</code>, and <code>eval_multi()</code> work normally, but <code>build()</code> cannot be called again without re-supplying the function.</p>"},{"location":"user-guide/tensor-train/#limitations","title":"Limitations","text":"<ul> <li>No analytical derivatives. The TT format does not support spectral   differentiation matrices. Derivatives are computed via finite differences, which   are less accurate (especially for second-order derivatives like Gamma).</li> <li>Error estimates are approximate. The <code>error_estimate()</code> method captures   per-core coefficient truncation but not rank truncation error. Always validate   against known solutions when possible.</li> <li>Convergence depends on function structure. TT-Cross works best for functions   with low-rank structure (smooth, with moderate coupling between variables). Not   all functions have good low-rank TT approximations -- highly oscillatory or   discontinuous functions may require very high ranks.</li> <li>Build cost grows with rank. While \\(O(d \\cdot n \\cdot r^2)\\) is much better   than \\(O(n^d)\\), a large <code>max_rank</code> (say, 50+) can still be expensive for costly   functions.</li> </ul>"},{"location":"user-guide/tensor-train/#mathematical-reference","title":"Mathematical Reference","text":"<p>The TT interpolation approach implemented here follows:</p> <ul> <li>Ruiz &amp; Zeron (2021), Machine Learning for Risk Calculations, Wiley Finance,   Chapter 6: Tensor Train decomposition for Chebyshev interpolation.</li> <li>Oseledets &amp; Tyrtyshnikov (2010), \"TT-Cross approximation for multidimensional   arrays\" -- the cross approximation algorithm used to build TT cores from function   evaluations.</li> <li>Goreinov, Tyrtyshnikov &amp; Zamarashkin (1997) -- maxvol algorithm for pivot   selection within TT-Cross.</li> </ul>"},{"location":"user-guide/tensor-train/#api-reference","title":"API Reference","text":"<p>Chebyshev interpolation in Tensor Train format.</p> <p>For functions of 5+ dimensions where full tensor interpolation is infeasible. Uses TT-Cross to build from O(d * n * r^2) function evaluations instead of O(n^d), then evaluates via TT inner product.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>callable</code> <p>Function to approximate. Signature: <code>f(point, data) -&gt; float</code> where <code>point</code> is a list of floats and <code>data</code> is arbitrary additional data (can be None).</p> required <code>num_dimensions</code> <code>int</code> <p>Number of input dimensions.</p> required <code>domain</code> <code>list of (float, float)</code> <p>Bounds [(lo, hi), ...] for each dimension.</p> required <code>n_nodes</code> <code>list of int</code> <p>Number of Chebyshev nodes per dimension.</p> required <code>max_rank</code> <code>int</code> <p>Maximum TT rank. Higher = more accurate, more expensive. Default is 10.</p> <code>10</code> <code>tolerance</code> <code>float</code> <p>Convergence tolerance for TT-Cross. Default is 1e-6.</p> <code>1e-06</code> <code>max_sweeps</code> <code>int</code> <p>Maximum number of TT-Cross sweeps. Default is 10.</p> <code>10</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; def f(x, _):\n...     return math.sin(x[0]) + math.sin(x[1]) + math.sin(x[2])\n&gt;&gt;&gt; tt = ChebyshevTT(f, 3, [[-1, 1], [-1, 1], [-1, 1]], [11, 11, 11])\n&gt;&gt;&gt; tt.build(verbose=False)\n&gt;&gt;&gt; tt.eval([0.5, 0.3, 0.1])\n0.8764...\n</code></pre>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.tt_ranks","title":"<code>tt_ranks</code>  <code>property</code>","text":"<p>TT ranks [1, r_1, r_2, ..., r_{d-1}, 1].</p> <p>Returns:</p> Type Description <code>list of int</code> <p>The TT rank vector. Only available after :meth:<code>build</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.compression_ratio","title":"<code>compression_ratio</code>  <code>property</code>","text":"<p>Ratio of full tensor elements to TT storage elements.</p> <p>Returns:</p> Type Description <code>float</code> <p>Compression ratio (&gt; 1 means TT is more compact).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.total_build_evals","title":"<code>total_build_evals</code>  <code>property</code>","text":"<p>Total number of function evaluations used during build.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of function evaluations. Only meaningful after :meth:<code>build</code>.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.build","title":"<code>build(verbose=True, seed=None, method='cross')</code>","text":"<p>Build TT approximation and convert to Chebyshev coefficient cores.</p> <p>The build process has three stages:</p> <ol> <li>Generate Chebyshev grids. Compute Type I Chebyshev nodes    in each dimension, scaled to the specified domain.</li> <li>Build value cores. Either TT-Cross (evaluating at    \\(O(d \\cdot n \\cdot r^2)\\) strategically selected points) or    TT-SVD (evaluating the full \\(O(n^d)\\) tensor, then decomposing    via sequential SVD).</li> <li>Convert to coefficient cores. Apply DCT-II along the node    axis of each core to convert from function values at Chebyshev    nodes to Chebyshev expansion coefficients. This enables    evaluation at arbitrary (non-grid) points via the Chebyshev    polynomial inner product.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print build progress. Default is True.</p> <code>True</code> <code>seed</code> <code>int or None</code> <p>Random seed for TT-Cross initialization. Default is None. Ignored when <code>method='svd'</code>.</p> <code>None</code> <code>method</code> <code>``'cross'`` or ``'svd'``</code> <p>Build algorithm. <code>'cross'</code> (default) uses TT-Cross to evaluate the function at \\(O(d \\cdot n \\cdot r^2)\\) strategically selected points. <code>'svd'</code> builds the full tensor and decomposes via truncated SVD -- only feasible for moderate dimensions (\\(d \\leq 6\\)) but useful for validation.</p> <code>'cross'</code>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.eval","title":"<code>eval(point)</code>","text":"<p>Evaluate at a single point via TT inner product.</p> <p>Computes the Chebyshev interpolant value at an arbitrary point by contracting the pre-computed coefficient cores with Chebyshev polynomial values. For each dimension \\(k\\):</p> <ol> <li>Scale the query coordinate to \\([-1, 1]\\).</li> <li>Evaluate all Chebyshev polynomials \\(T_0, \\ldots, T_{n_k-1}\\).</li> <li>Contract with the coefficient core:    \\(v = \\sum_j q_j \\cdot \\text{core}[:, j, :]\\)</li> </ol> <p>The chain of contractions reduces to a scalar. Cost: \\(O(d \\cdot n \\cdot r^2)\\) per point.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Query point, one coordinate per dimension.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Interpolated value at the query point.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.eval_batch","title":"<code>eval_batch(points)</code>","text":"<p>Evaluate at multiple points simultaneously.</p> <p>Vectorizes the TT inner product over all N points using <code>np.einsum</code> for batched matrix contractions. For each dimension, all N polynomial vectors are contracted with the coefficient core in a single einsum call, then all N chain multiplications proceed in parallel. Typical speedup is 15--20x over calling :meth:<code>eval</code> in a loop.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray of shape (N, num_dimensions)</code> <p>Query points.</p> required <p>Returns:</p> Type Description <code>ndarray of shape (N,)</code> <p>Interpolated values at each query point.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.eval_multi","title":"<code>eval_multi(point, derivative_orders)</code>","text":"<p>Evaluate with finite-difference derivatives at a single point.</p> <p>Uses central finite differences. The first entry in <code>derivative_orders</code> is typically <code>[0, 0, ..., 0]</code> for the function value; subsequent entries specify derivative orders per dimension.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>list of float</code> <p>Evaluation point in the full n-dimensional space.</p> required <code>derivative_orders</code> <code>list of list of int</code> <p>Each inner list specifies derivative order per dimension. Supports 0 (value), 1 (first derivative), and 2 (second derivative).</p> required <p>Returns:</p> Type Description <code>list of float</code> <p>One result per derivative order specification.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.error_estimate","title":"<code>error_estimate()</code>","text":"<p>Estimate interpolation error from Chebyshev coefficient cores.</p> <p>For each dimension d, takes the maximum magnitude of the last Chebyshev coefficient across all \"rows\" and \"columns\" of the core (i.e., max over left-rank and right-rank indices of <code>|core[:, -1, :]|</code>). Returns the sum across dimensions.</p> <p>This is an approximate analog of the ex ante error estimation from Ruiz &amp; Zeron (2021), Section 3.4, adapted for TT format.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated interpolation error.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Return picklable state, excluding the original function.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore state from a pickled dict.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.save","title":"<code>save(path)</code>","text":"<p>Save the built TT interpolant to a file.</p> <p>The original function is not saved -- only the numerical data needed for evaluation. The saved file can be loaded with :meth:<code>load</code> without access to the original function.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Destination file path.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>build()</code> has not been called.</p>"},{"location":"user-guide/tensor-train/#pychebyshev.tensor_train.ChebyshevTT.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load a previously saved TT interpolant from a file.</p> <p>The loaded object can evaluate immediately; no rebuild is needed. The <code>function</code> attribute will be <code>None</code>. Assign a new function before calling <code>build()</code> again if a rebuild is desired.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or path - like</code> <p>Path to the saved file.</p> required <p>Returns:</p> Type Description <code>ChebyshevTT</code> <p>The restored TT interpolant.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the file was saved with a different PyChebyshev version.</p> <code>.. warning::</code> <p>This method uses :mod:<code>pickle</code> internally. Pickle can execute arbitrary code during deserialization. Only load files you trust.</p>"},{"location":"user-guide/usage/","title":"Usage Patterns","text":""},{"location":"user-guide/usage/#basic-workflow","title":"Basic Workflow","text":"<p>Every PyChebyshev workflow follows three steps:</p> <ol> <li>Define a callable function</li> <li>Build the interpolant (evaluates at Chebyshev nodes, pre-computes weights)</li> <li>Query at arbitrary points</li> </ol> <pre><code>from pychebyshev import ChebyshevApproximation\n\ncheb = ChebyshevApproximation(func, num_dimensions, domain, n_nodes)\ncheb.build()\nresult = cheb.vectorized_eval(point, derivative_order)\n</code></pre>"},{"location":"user-guide/usage/#evaluation-methods","title":"Evaluation Methods","text":"<p>PyChebyshev provides several evaluation methods with different speed/safety tradeoffs:</p> Method Speed Safety Use When <code>eval()</code> Slowest Full validation Testing and debugging <code>vectorized_eval()</code> Fastest Full validation Default choice <code>vectorized_eval_multi()</code> Fastest (multi) Full validation Price + Greeks at same point <p>Why no JIT?</p> <p>Earlier versions offered a Numba JIT <code>fast_eval()</code> path, but <code>vectorized_eval()</code> is ~150x faster because it routes N-D tensor contractions through BLAS GEMV \u2014 a single optimized matrix-vector multiply per dimension. JIT compilation cannot beat BLAS for this workload. <code>fast_eval()</code> is deprecated and will be removed in a future version.</p>"},{"location":"user-guide/usage/#vectorized_eval-recommended","title":"<code>vectorized_eval()</code> \u2014 Recommended","text":"<p>Uses BLAS matrix-vector products. For 5D with 11 nodes, replaces 16,105 Python loop iterations with 5 BLAS calls:</p> <pre><code>price = cheb.vectorized_eval([100, 100, 1.0, 0.25, 0.05], [0, 0, 0, 0, 0])\n</code></pre>"},{"location":"user-guide/usage/#vectorized_eval_multi-best-for-multiple-derivatives","title":"<code>vectorized_eval_multi()</code> \u2014 Best for multiple derivatives","text":"<p>Pre-computes normalized barycentric weights once, reuses across all derivative orders:</p> <pre><code>results = cheb.vectorized_eval_multi(\n    [100, 100, 1.0, 0.25, 0.05],\n    [\n        [0, 0, 0, 0, 0],  # price\n        [1, 0, 0, 0, 0],  # delta (dV/dS)\n        [2, 0, 0, 0, 0],  # gamma (d\u00b2V/dS\u00b2)\n        [0, 0, 0, 1, 0],  # vega  (dV/d\u03c3)\n        [0, 0, 0, 0, 1],  # rho   (dV/dr)\n    ],\n)\nprice, delta, gamma, vega, rho = results\n</code></pre>"},{"location":"user-guide/usage/#batch-evaluation","title":"Batch Evaluation","text":"<p>For evaluating at many points:</p> <pre><code>import numpy as np\n\npoints = np.array([\n    [100, 100, 1.0, 0.25, 0.05],\n    [110, 100, 1.0, 0.25, 0.05],\n    [90, 100, 1.0, 0.25, 0.05],\n])\nprices = cheb.vectorized_eval_batch(points, [0, 0, 0, 0, 0])\n</code></pre>"},{"location":"user-guide/usage/#function-signature","title":"Function Signature","text":"<p>The function passed to <code>ChebyshevApproximation</code> must accept:</p> <ul> <li><code>point</code> \u2014 a list of floats (one per dimension)</li> <li><code>data</code> \u2014 arbitrary additional data (use <code>None</code> if not needed)</li> </ul> <pre><code>def my_func(point, data):\n    x, y, z = point\n    return x**2 + y * z\n</code></pre>"},{"location":"user-guide/usage/#next-steps","title":"Next Steps","text":"<ul> <li>Computing Greeks -- analytical derivatives via spectral differentiation</li> <li>Error Estimation -- check accuracy without test points</li> <li>Saving &amp; Loading -- persist built interpolants for production</li> <li>For 5+ dimensions, see Tensor Train or Sliding Technique</li> </ul>"}]}